From 2973ac33a413c00020a6f5dec01498d9fff909c3 Mon Sep 17 00:00:00 2001
From: y00945504 <yuhui87@huawei.com>
Date: Thu, 21 Aug 2025 10:46:38 +0800
Subject: [PATCH] vllm v0.9.1 adapt patch

---
 vllm/v1/core/sched/scheduler.py    | 3 +++
 vllm/v1/outputs.py                 | 1 +
 vllm/v1/request.py                 | 1 +
 vllm/v1/worker/gpu_model_runner.py | 7 ++++---
 4 files changed, 9 insertions(+), 3 deletions(-)

diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 3d7bbe7e0..1e382e319 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -761,6 +761,9 @@ class Scheduler(SchedulerInterface):
             new_logprobs = None
             new_token_ids = generated_token_ids
             kv_transfer_params = None
+            
+            if model_runner_output.finished_dumping is not None:
+                request.succeed_dumped_blocks.extend(model_runner_output.finished_dumping.get(req_id, []))
 
             # Append generated tokens and check for stop. Note that if
             # a request is still being prefilled, we expect the model runner
diff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py
index 17a299d57..d6c767f78 100644
--- a/vllm/v1/outputs.py
+++ b/vllm/v1/outputs.py
@@ -104,6 +104,7 @@ class ModelRunnerOutput:
     # [req_ids]
     finished_sending: Optional[set[str]] = None
     finished_recving: Optional[set[str]] = None
+    finished_dumping: Optional[dict[str, list[str]]] = None
 
 
 EMPTY_MODEL_RUNNER_OUTPUT = ModelRunnerOutput(req_ids=[],
diff --git a/vllm/v1/request.py b/vllm/v1/request.py
index 53fd70fab..ff22263fd 100644
--- a/vllm/v1/request.py
+++ b/vllm/v1/request.py
@@ -82,6 +82,7 @@ class Request:
         # State
         # The number of tokens with prefix cache hits.
         self.num_cached_tokens = -1
+        self.succeed_dumped_blocks: list[str] = []
 
     @classmethod
     def from_engine_core_request(cls, request: EngineCoreRequest) -> "Request":
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index b1bc727e1..5d2aebb5c 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -1264,7 +1264,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 inputs_embeds=inputs_embeds,
             )
 
-            self.maybe_wait_for_kv_save()
+            finished_dumping = self.maybe_wait_for_kv_save()
             finished_sending, finished_recving = (
                 self.get_finished_kv_transfers(scheduler_output))
 
@@ -1505,6 +1505,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             prompt_logprobs_dict=prompt_logprobs_dict,
             finished_sending=finished_sending,
             finished_recving=finished_recving,
+            finished_dumping=finished_dumping
         )
 
     def kv_connector_no_forward(
@@ -1540,9 +1541,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             kv_connector.start_load_kv(get_forward_context())
 
     @staticmethod
-    def maybe_wait_for_kv_save() -> None:
+    def maybe_wait_for_kv_save() -> Optional[dict[str, list[str]]]:
         if has_kv_transfer_group():
-            get_kv_transfer_group().wait_for_save()
+            return get_kv_transfer_group().wait_for_save()
 
     @staticmethod
     def get_finished_kv_transfers(
-- 
2.50.1.windows.1

