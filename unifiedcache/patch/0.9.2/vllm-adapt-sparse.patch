From 214bfdba485755ff26f09759d4ea29d4d3908d51 Mon Sep 17 00:00:00 2001
From: hek14 <1023129548@qq.com>
Date: Mon, 25 Aug 2025 15:43:52 +0800
Subject: [PATCH] ucm_sparse patch

---
 vllm/v1/core/kv_cache_manager.py | 30 +++++++++++++++++++++++++++++-
 vllm/v1/core/sched/output.py     |  3 +++
 vllm/v1/core/sched/scheduler.py  | 30 ++++++++++++++++++++++++++++--
 vllm/v1/worker/block_table.py    | 13 +++++++++++++
 4 files changed, 73 insertions(+), 3 deletions(-)

diff --git a/vllm/v1/core/kv_cache_manager.py b/vllm/v1/core/kv_cache_manager.py
index 6937455e7..bdd2b1ece 100644
--- a/vllm/v1/core/kv_cache_manager.py
+++ b/vllm/v1/core/kv_cache_manager.py
@@ -3,7 +3,7 @@
 
 from collections import defaultdict
 from dataclasses import dataclass
-from typing import Optional
+from typing import Optional, Union
 
 from vllm.distributed.kv_events import KVCacheEvent
 from vllm.logger import init_logger
@@ -14,6 +14,9 @@ from vllm.v1.core.kv_cache_utils import (BlockHash, KVCacheBlock,
 from vllm.v1.kv_cache_interface import KVCacheConfig
 from vllm.v1.metrics.stats import PrefixCacheStats
 from vllm.v1.request import Request, RequestStatus
+from unifiedcache.integration.vllm.ucm_sparse.state import get_ucm_sparse, has_ucm_sparse
+from unifiedcache.integration.vllm.ucm_sparse.base import INVALID_SLOT
+import math
 
 logger = init_logger(__name__)
 
@@ -193,6 +196,7 @@ class KVCacheManager:
         num_draft_tokens: int = 0,
         num_lookahead_tokens: int = 0,
         delay_cache_blocks: bool = False,
+        num_slots_sparsed: Union[None, int] = None
     ) -> Optional[KVCacheBlocks]:
         """Add slots for a request with new tokens to append.
 
@@ -231,6 +235,30 @@ class KVCacheManager:
         """
         if num_new_tokens == 0:
             raise ValueError("num_new_tokens must be greater than 0")
+        if num_slots_sparsed != INVALID_SLOT:
+            num_blocks_need = math.ceil(num_slots_sparsed / self.block_size)
+            allocated_blocks = self.coordinator.get_blocks(request.request_id)[0]
+            returned_blocks = []
+            sparsed_blocks = []
+            for i, block in enumerate(allocated_blocks):
+                if i < num_blocks_need:
+                    sparsed_blocks.append(block)
+                else:
+                    returned_blocks.append(block)
+                self.block_pool._maybe_evict_cached_block(block)
+            self.block_pool.free_blocks(returned_blocks)
+            self.coordinator.single_type_managers[0].req_to_blocks[request.request_id] = sparsed_blocks
+            new_computed_block_list = tuple(
+                [] for _ in range(len(self.kv_cache_config.kv_cache_groups)))
+            num_blocks_to_allocate = self.coordinator.get_num_blocks_to_allocate(
+                request_id=request.request_id,
+                num_tokens=num_slots_sparsed,
+                new_computed_blocks=new_computed_block_list,
+            )
+            if num_blocks_to_allocate > self.block_pool.get_num_free_blocks():
+                return None
+            new_blocks = self.coordinator.allocate_new_blocks(request.request_id, num_slots_sparsed)
+            return KVCacheBlocks(tuple([sparsed_blocks]))
 
         if new_computed_blocks is not None:
             new_computed_block_list = new_computed_blocks.blocks
diff --git a/vllm/v1/core/sched/output.py b/vllm/v1/core/sched/output.py
index d34f39327..141d750b3 100644
--- a/vllm/v1/core/sched/output.py
+++ b/vllm/v1/core/sched/output.py
@@ -155,3 +155,6 @@ class SchedulerOutput:
 
     # KV Cache Connector metadata.
     kv_connector_metadata: Optional[KVConnectorMetadata] = None
+
+    # modified slots by sparse algorithm
+    req_sparsed_slots: dict[str, int] = None
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 22c0ad8d6..9fb71350f 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -35,6 +35,9 @@ from vllm.v1.request import Request, RequestStatus
 from vllm.v1.spec_decode.metrics import SpecDecodingStats
 from vllm.v1.structured_output import StructuredOutputManager
 
+from unifiedcache.integration.vllm.ucm_sparse.state import ensure_ucm_sparse_initialized, get_ucm_sparse, has_ucm_sparse
+from unifiedcache.integration.vllm.ucm_sparse.base import UcmSparseBase, UcmSparseRole, INVALID_SLOT
+
 logger = init_logger(__name__)
 
 
@@ -79,12 +82,18 @@ class Scheduler(SchedulerInterface):
         # will have a corresponding KVConnector with Role=WORKER.
         # KV Connector pushes/pull of remote KVs for P/D and offloading.
         self.connector = None
+        self.ucm_sparse = None
         if self.vllm_config.kv_transfer_config is not None:
             assert len(self.kv_cache_config.kv_cache_groups) == 1, (
                 "Multiple KV cache groups are not currently supported "
                 "with KV connectors")
             self.connector = KVConnectorFactory.create_connector_v1(
                 config=self.vllm_config, role=KVConnectorRole.SCHEDULER)
+            # Initialize UCM Sparse if available
+            if "ucm_sparse_method" in vllm_config.kv_transfer_config.kv_connector_extra_config:
+                ensure_ucm_sparse_initialized(vllm_config, role=UcmSparseRole.SCHEDULER)
+                self.ucm_sparse = get_ucm_sparse()
+                logger.info("UCM Sparse initialized successfully: {}".format(self.ucm_sparse))
 
         self.kv_event_publisher = EventPublisherFactory.create(
             self.kv_events_config,
@@ -201,8 +210,13 @@ class Scheduler(SchedulerInterface):
 
         # First, schedule the RUNNING requests.
         req_index = 0
+        req_sparsed_slots: dict[str, int] = {}
         while req_index < len(self.running) and token_budget > 0:
             request = self.running[req_index]
+            num_slots_sparsed = INVALID_SLOT
+            if self.ucm_sparse:
+                num_slots_sparsed = self.ucm_sparse.estimate_num_slots_sparsed(request)
+            req_sparsed_slots.update({request.request_id: num_slots_sparsed})
 
             num_new_tokens = (request.num_tokens_with_spec -
                               request.num_computed_tokens)
@@ -250,7 +264,9 @@ class Scheduler(SchedulerInterface):
                     request,
                     num_new_tokens,
                     num_draft_tokens=num_draft_tokens,
-                    num_lookahead_tokens=self.num_lookahead_tokens)
+                    num_lookahead_tokens=self.num_lookahead_tokens,
+                    num_slots_sparsed=num_slots_sparsed
+                    )
                 if new_blocks is None:
                     # The request cannot be scheduled.
                     # Preempt the lowest-priority request.
@@ -337,6 +353,10 @@ class Scheduler(SchedulerInterface):
                     break
 
                 request = self.waiting.peek_request()
+                num_slots_sparsed = INVALID_SLOT
+                if self.ucm_sparse:
+                    num_slots_sparsed = self.ucm_sparse.estimate_num_slots_sparsed(request)
+                req_sparsed_slots.update({request.request_id: num_slots_sparsed})
 
                 # KVTransfer: skip request if still waiting for remote kvs.
                 if request.status == RequestStatus.WAITING_FOR_REMOTE_KVS:
@@ -446,6 +466,7 @@ class Scheduler(SchedulerInterface):
                     new_computed_blocks,
                     num_lookahead_tokens=self.num_lookahead_tokens,
                     delay_cache_blocks=load_kv_async,
+                    num_slots_sparsed=num_slots_sparsed
                 )
                 if new_blocks is None:
                     # The request cannot be scheduled.
@@ -559,6 +580,7 @@ class Scheduler(SchedulerInterface):
             scheduled_spec_decode_tokens=scheduled_spec_decode_tokens,
             scheduled_encoder_inputs=scheduled_encoder_inputs,
             num_common_prefix_blocks=num_common_prefix_blocks,
+            req_sparsed_slots=req_sparsed_slots,
             # finished_req_ids is an existing state in the scheduler,
             # instead of being newly scheduled in this step.
             # It contains the request IDs that are finished in between
@@ -844,6 +866,7 @@ class Scheduler(SchedulerInterface):
                         spec_token_ids[req_index])
                 else:
                     request.spec_token_ids = spec_token_ids[req_index]
+
             # Get prompt logprobs for this request.
             prompt_logprobs_tensors = prompt_logprobs_dict.get(req_id)
             if new_token_ids or pooler_output is not None \
@@ -929,6 +952,8 @@ class Scheduler(SchedulerInterface):
     def add_request(self, request: Request) -> None:
         self.waiting.add_request(request)
         self.requests[request.request_id] = request
+        if self.ucm_sparse:
+            self.ucm_sparse.request_begin(request.request_id, request.prompt_token_ids)
         if self.log_stats:
             request.record_event(EngineCoreEventType.QUEUED)
 
@@ -978,7 +1003,8 @@ class Scheduler(SchedulerInterface):
 
     def _free_request(self, request: Request) -> Optional[dict[str, Any]]:
         assert request.is_finished()
-
+        if self.ucm_sparse:
+            self.ucm_sparse.request_finished_in_scheduler(request.request_id)
         delay_free_blocks, kv_xfer_params = self._connector_finished(request)
         self.encoder_cache_manager.free(request)
         request_id = request.request_id
diff --git a/vllm/v1/worker/block_table.py b/vllm/v1/worker/block_table.py
index 8f4e8d64c..f45e39f5c 100644
--- a/vllm/v1/worker/block_table.py
+++ b/vllm/v1/worker/block_table.py
@@ -61,6 +61,15 @@ class BlockTable:
         self.num_blocks_per_row[row_idx] += num_blocks
         self.block_table_np[row_idx, start:start + num_blocks] = block_ids
 
+    def reset_row(
+            self,
+            row_idx: int,
+    ) -> None:
+        self.num_blocks_per_row[row_idx] = 0
+        self.block_table[row_idx].fill_(0)
+        self.block_table_cpu[row_idx].fill_(0)
+        self.block_table_np[row_idx].fill(0)
+
     def add_row(self, block_ids: list[int], row_idx: int) -> None:
         self.num_blocks_per_row[row_idx] = 0
         self.append_row(block_ids, row_idx)
@@ -117,6 +126,10 @@ class MultiGroupBlockTable:
         for i, block_table in enumerate(self.block_tables):
             block_table.append_row(block_ids[i], row_idx)
 
+    def reset_row(self, row_idx: int) -> None:
+        for i, block_table in enumerate(self.block_tables):
+            block_table.reset_row(row_idx)
+
     def add_row(self, block_ids: tuple[list[int], ...], row_idx: int) -> None:
         for i, block_table in enumerate(self.block_tables):
             block_table.add_row(block_ids[i], row_idx)
-- 
2.50.1.windows.1

