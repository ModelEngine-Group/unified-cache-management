From 32e09eba672e6f55b67962bfef63eca9b67fe508 Mon Sep 17 00:00:00 2001
From: hek14 <1023129548@qq.com>
Date: Mon, 25 Aug 2025 15:48:40 +0800
Subject: [PATCH] ucm_sparse patch

---
 vllm_ascend/attention/attention_v1.py | 42 ++++++++++++++++++
 vllm_ascend/worker/model_runner_v1.py | 63 +++++++++++++++++++++++----
 vllm_ascend/worker/worker_v1.py       |  2 +
 3 files changed, 98 insertions(+), 9 deletions(-)

diff --git a/vllm_ascend/attention/attention_v1.py b/vllm_ascend/attention/attention_v1.py
index 915feb7..0600f35 100644
--- a/vllm_ascend/attention/attention_v1.py
+++ b/vllm_ascend/attention/attention_v1.py
@@ -36,6 +36,8 @@ from vllm_ascend.ops.attention import vanilla_chunked_prefill
 from vllm_ascend.utils import (ACL_FORMAT_FRACTAL_NZ, aligned_16, is_310p,
                                nd_to_nz_2d, nd_to_nz_spec)
 
+from ucm.integration.vllm.ucm_sparse.state import get_ucm_sparse, has_ucm_sparse
+
 
 class AscendAttentionBackend(AttentionBackend):
     accept_output_buffer: bool = True
@@ -453,6 +455,8 @@ def unified_ascend_attention_with_output(
     attn_metadata = forward_context.attn_metadata
     self = forward_context.no_compile_layers[layer_name]
     kv_cache = self.kv_cache[forward_context.virtual_engine]
+
+    maybe_execute_sparse_attention_begin(query, key, value, layer_name, forward_context)
     self.impl.forward(self,
                       query,
                       key,
@@ -461,6 +465,7 @@ def unified_ascend_attention_with_output(
                       attn_metadata,
                       output,
                       trace_flag=False)
+    maybe_execute_sparse_attention_finished(query, key, value, output, layer_name, forward_context)
     maybe_save_kv_layer_to_connector(layer_name, kv_cache)
     return
 
@@ -492,6 +497,43 @@ def maybe_save_kv_layer_to_connector(
     connector.save_kv_layer(layer_name, kv_cache_layer,
                             attn_metadata)
 
+def maybe_execute_sparse_attention_begin(
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        layer_name: str,
+        forward_context: ForwardContext,
+):
+    if not has_ucm_sparse():
+        return
+
+    ucm_sparse = get_ucm_sparse()
+
+    attn_metadata = forward_context.attn_metadata
+    if attn_metadata is None:
+        return
+
+    ucm_sparse.attention_begin(query, key, value, layer_name, forward_context)
+
+def maybe_execute_sparse_attention_finished(
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        attn_output: torch.Tensor,
+        layer_name: str,
+        forward_context: ForwardContext,
+):
+    if not has_ucm_sparse():
+        return
+
+    ucm_sparse = get_ucm_sparse()
+
+    attn_metadata = forward_context.attn_metadata
+    if attn_metadata is None:
+        return
+
+    ucm_sparse.attention_finished(query, key, value, attn_output, layer_name, forward_context)
+
 def unified_attention_with_output_fake(
     query: torch.Tensor,
     key: torch.Tensor,
diff --git a/vllm_ascend/worker/model_runner_v1.py b/vllm_ascend/worker/model_runner_v1.py
index f9cca93..31766b2 100644
--- a/vllm_ascend/worker/model_runner_v1.py
+++ b/vllm_ascend/worker/model_runner_v1.py
@@ -91,6 +91,9 @@ from vllm_ascend.worker.eagle_proposer_v1 import EagleProposer
 from vllm_ascend.worker.mtp_proposer_v1 import MtpProposer
 from vllm_ascend.worker.npu_input_batch import CachedRequestState, InputBatch
 
+from ucm.integration.vllm.ucm_sparse.state import get_ucm_sparse, has_ucm_sparse
+from ucm.integration.vllm.ucm_sparse.base import UcmSparseMetadata, INVALID_SLOT
+
 if TYPE_CHECKING:
     import xgrammar as xgr  # type: ignore[import-untyped]
     from vllm.v1.core.sched.output import SchedulerOutput
@@ -350,6 +353,7 @@ class NPUModelRunner(LoRAModelRunnerMixin):
         """
         # Remove finished requests from the cached states.
         for req_id in scheduler_output.finished_req_ids:
+            self.ucm_sparse_request_finished_in_worker(req_id)
             self.requests.pop(req_id, None)
             self.encoder_cache.pop(req_id, None)
         # Remove the finished requests from the persistent batch.
@@ -456,12 +460,14 @@ class NPUModelRunner(LoRAModelRunnerMixin):
 
         # Update the states of the running/resumed requests.
         req_data = scheduler_output.scheduled_cached_reqs
+        req_sparsed_slots = scheduler_output.req_sparsed_slots
         is_last_rank = get_pp_group().is_last_rank
         for i, req_id in enumerate(req_data.req_ids):
             req_state = self.requests[req_id]
             num_computed_tokens = req_data.num_computed_tokens[i]
             new_block_ids = req_data.new_block_ids[i]
             resumed_from_preemption = req_data.resumed_from_preemption[i]
+            is_sparsed_request = req_sparsed_slots[req_id] != INVALID_SLOT
 
             req_state.num_computed_tokens = num_computed_tokens
             if not is_last_rank:
@@ -477,15 +483,15 @@ class NPUModelRunner(LoRAModelRunnerMixin):
                     req_state.output_token_ids.extend(
                         new_token_ids[-num_new_tokens:])
             # Update the block IDs.
-            if not resumed_from_preemption:
+            if resumed_from_preemption or is_sparsed_request:
+                # The request is resumed from preemption.
+                # Replace the existing block IDs with the new ones.
+                req_state.block_ids = new_block_ids
+            else:
                 # Append the new blocks to the existing block IDs.
                 for block_ids, new_ids in zip(  # type: ignore[call-overload]
                         req_state.block_ids, new_block_ids):
                     block_ids.extend(new_ids)
-            else:
-                # The request is resumed from preemption.
-                # Replace the existing block IDs with the new ones.
-                req_state.block_ids = new_block_ids
 
             req_index = self.input_batch.req_id_to_index.get(req_id)
             if req_index is None:
@@ -499,6 +505,9 @@ class NPUModelRunner(LoRAModelRunnerMixin):
             self.input_batch.num_computed_tokens_cpu[req_index] = (
                 num_computed_tokens)
 
+            if is_sparsed_request:
+                self.input_batch.block_table.reset_row(req_index)
+
             self.input_batch.block_table.append_row(new_block_ids, req_index)
 
             if not is_last_rank:
@@ -959,12 +968,21 @@ class NPUModelRunner(LoRAModelRunnerMixin):
             num_scheduled_tokens)
         seq_lens = self.seq_lens_cpu[:num_reqs]
 
+        # TODO: improve performance, no `positions_np.copy()`
+        sparsed_positions = positions_np.copy()
+        req_sparsed_slots = scheduler_output.req_sparsed_slots
+        for req_id in self.input_batch.req_id_to_index:
+            is_sparsed_request = req_sparsed_slots[req_id] != INVALID_SLOT
+            req_index = self.input_batch.req_id_to_index[req_id]
+            if is_sparsed_request:
+                sparsed_positions[req_index] -= seq_lens[req_index] - req_sparsed_slots[req_id]
+
         block_table_indices = (req_indices * self.max_num_blocks_per_req +
-                               positions_np // self.block_size)
+                               sparsed_positions // self.block_size)
 
         block_table_cpu = self.input_batch.block_table[0].get_cpu_tensor()
         block_numbers = block_table_cpu.flatten()[block_table_indices].numpy()
-        block_offsets = positions_np % self.block_size
+        block_offsets = sparsed_positions % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,
                out=self.slot_mapping_np[:total_num_scheduled_tokens])
@@ -989,10 +1007,16 @@ class NPUModelRunner(LoRAModelRunnerMixin):
         else:
             attn_state = AscendAttentionState.PrefillCacheHit
 
+        for req_id in self.input_batch.req_id_to_index:
+            is_sparsed_request = req_sparsed_slots[req_id] != INVALID_SLOT
+            req_index = self.input_batch.req_id_to_index[req_id]
+            if is_sparsed_request:
+                seq_lens[req_index] = req_sparsed_slots[req_id]
+
         self.attn_mask = self._make_attention_mask(
             seq_lens=seq_lens,
             query_lens=num_scheduled_tokens,
-            position=positions,
+            position=torch.tensor(sparsed_positions).npu(),
             attn_state=attn_state)
         self.attn_state = attn_state  # type: ignore
 
@@ -1131,6 +1155,7 @@ class NPUModelRunner(LoRAModelRunnerMixin):
                     maybe_converting_weight_acl_format(self.model,
                                                        ACL_FORMAT_FRACTAL_ND)
                     self.maybe_setup_kv_connector(scheduler_output)
+                    self.maybe_execute_ucm_sparse_begin(scheduler_output)
 
                     hidden_states = self.model(
                         input_ids=input_ids,
@@ -1140,6 +1165,7 @@ class NPUModelRunner(LoRAModelRunnerMixin):
                         **model_kwargs,
                     )
                     finished_dumping = self.maybe_wait_for_kv_save()
+                    self.maybe_execute_ucm_sparse_finished()
 
         use_spec_decode = len(
             scheduler_output.scheduled_spec_decode_tokens) > 0
@@ -2377,7 +2403,7 @@ class NPUModelRunner(LoRAModelRunnerMixin):
             if batch_size <= padded_batch_size < selected_batch_size:
                 selected_batch_size = padded_batch_size
         return selected_batch_size
-    
+
     @staticmethod
     def maybe_setup_kv_connector(scheduler_output: "SchedulerOutput"):
         # Update KVConnector with the KVConnector metadata forward().
@@ -2398,3 +2424,22 @@ class NPUModelRunner(LoRAModelRunnerMixin):
     def maybe_wait_for_kv_save():
         if has_kv_transfer_group():
             return get_kv_transfer_group().wait_for_save()
+
+    def maybe_execute_ucm_sparse_begin(self, scheduler_output: "SchedulerOutput"):
+        if not has_ucm_sparse():
+            return
+        ucm_sparse = get_ucm_sparse()
+        ucm_sparse.build_sparse_meta(scheduler_output, self.requests, self.input_batch)
+        ucm_sparse.execute_begin(scheduler_output)
+
+    def maybe_execute_ucm_sparse_finished(self):
+        if not has_ucm_sparse():
+            return
+        ucm_sparse = get_ucm_sparse()
+        ucm_sparse.execute_finished()
+
+    def ucm_sparse_request_finished_in_worker(self, request_id: str | int):
+        if not has_ucm_sparse():
+            return
+        ucm_sparse = get_ucm_sparse()
+        ucm_sparse.request_finished_in_worker(request_id)
\ No newline at end of file
diff --git a/vllm_ascend/worker/worker_v1.py b/vllm_ascend/worker/worker_v1.py
index df03d50..6ea5bf3 100644
--- a/vllm_ascend/worker/worker_v1.py
+++ b/vllm_ascend/worker/worker_v1.py
@@ -49,6 +49,7 @@ from vllm_ascend.utils import (check_kv_cache_bytes_cache_exist,
                                read_kv_cache_bytes_from_file,
                                sleep_mode_enabled, try_register_lib)
 from vllm_ascend.worker.model_runner_v1 import NPUModelRunner
+from ucm.integration.vllm.ucm_sparse.state import ensure_ucm_sparse_initialized
 
 
 class NPUWorker(WorkerBase):
@@ -321,6 +322,7 @@ class NPUWorker(WorkerBase):
             parallel_config.world_size_across_dp,
         )
         ensure_kv_transfer_initialized(self.vllm_config)
+        ensure_ucm_sparse_initialized(self.vllm_config)
 
     def _init_profiler(self):
         # Torch profiler. Enabled and configured through env vars:
-- 
2.50.1.windows.1

