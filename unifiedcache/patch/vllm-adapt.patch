From 1d914eceea443bec4d3536c9ff86ccef2f4f6a7b Mon Sep 17 00:00:00 2001
From: hek14 <1023129548@qq.com>
Date: Tue, 19 Aug 2025 14:46:31 +0800
Subject: [PATCH] ucm_sparse patch

fix typo
---
 vllm-adapt.patch                   | Bin 0 -> 14174 bytes
 vllm/v1/core/kv_cache_manager.py   |  31 +++++++++++++++++++++++++++-
 vllm/v1/core/sched/output.py       |   3 +++
 vllm/v1/core/sched/scheduler.py    |  32 +++++++++++++++++++++++++++--
 vllm/v1/outputs.py                 |   1 +
 vllm/v1/request.py                 |   2 ++
 vllm/v1/worker/block_table.py      |  13 ++++++++++++
 vllm/v1/worker/gpu_model_runner.py |   7 ++++---
 8 files changed, 83 insertions(+), 6 deletions(-)
 create mode 100644 vllm-adapt.patch

diff --git a/vllm-adapt.patch b/vllm-adapt.patch
new file mode 100644
index 0000000000000000000000000000000000000000..605b1938b65cccd9b3e6f7c38148cf2200468547
GIT binary patch
literal 14174
zcmcheZBJas702gwrGAH%{RYeS#x^FHNK{T^$Bi626g#b|)LJdD5M&I5g}C|hZU4VH
zdzd>9cXt8XS}phPy)$Rd^V1Cf`|n;j4Ey>F!hL-X^y^&r<M37ZJsj!tA>KXI-`#K$
z&cZ}@Q{DfnyFJ~XgbU3$jem!_{-N1BanAK-|2h0gI2;T1B>YFP_QGm-Ec{l(hQ5!&
zdKl`n79QyPL~kZ~`at*$!b4pjgs=7NaR3VR+`l0$z_}YgH>c`pcq(k}3(rU4p70z9
zuHCQe-rgdML3kVf6}H0jFbdy=zv=p2co}{OufvP*O{8ov`#aEBKKqi(i6k|+l-W>x
zJCvNx_2y9WKt{-G7~X5lC_W#H%TvvrXpBEc#>e_h#L@F8@1e#&)cYOj#8l4*g7-mk
zMi<bOl>!&X>1Ftb@Ghaf5`=R>+S8ZoojCih;FOeH2n)Dzs2O|0W<1lmp=j9^1|wl}
zs;6fWtA6}`j-%gn#+EoBV_(0`%cDr`SX@B=e-j21tqS!yFMiax?eML>-BF9-e-OkY
z!2{Y!<js-R!V1i;rqxQXE@!;DobfVzjQltdZuc}}OEBTuM7DaOdo1cqxFPog-7m%q
zDKisH4#aWe_gGlg7;R-^w<0Dsl68{qugPxj$!1Oa*JKA~3pQdm;yjJ;-^?Vw7ik6e
zpLFNGrfmtg1sBa5NcUW0xG(i^AR2N_92WdAJE`qqRL%TGtNkS2AV1a_i-un!7WKNN
z?!D98O?_X-nI@CFg-t+HSAX%&N0J8?h`0V#7}fN^^<-Bqn^~5U#TL|};-+yW#TabE
zeAHE=OCJn)(~U9sK=ksFY-C3f=CQ1AsAoHhFnozUJpB|gW*$ygk2Le3xy=zjJ>J;;
z6H#JsTam;3eK)Sx)^1*BBFSvaCM{y*_G#XwAO3t#ntM>ZS^ngW^qvTEAy2VrCE^6}
zY!p2p7L6wy&g>W)+HLqVERI>v;$P;N7xa~vS(oUG<$)*k?g@u3U(5Ib>~coe=e-im
z_!qqQ+008M4qr85?hBGfd9t=Vmep4OeKy|w%k@|9%O~n}|EhJ#JkWyr?Y20Ie=seu
zzr^c<8JEnzB0GHLNMqBx75yGXjuH*Hw|vC%E4QC@S-;!Q6Xinp<O|$>=EtiQ`EAdX
z-HZL8^~lA}l-pQzzH^z?r7XnmZfl*7!kbKbAU{7=E=C46))zfK(}zriI8DZE8fp=+
zjy(p8c!4z;*EYjH6}z?-t)A;|83P?3@bIWwM^bW>si1<%SHcBK!S_TI4kCtRHPCEO
z@Fx3ua;EE+W<1jsz24K6Is2M@q9;yIA6DqZkwy<AFY>Il6;+)Yzoz+c&@;)A?yxVU
zltb}(el=b5Uw*X=KEd`XYBa-=jP^h{xAX9P6gCOBjwAn4mf*RMuUTHP7cj649qTw&
zo}O^>+{QDcd{xIS<=i7-OoeeFK22pK@P(`eTj;L<N~~^$l(n8pa;^{PvSTvHRzfe(
z@Wg@L=t+5oe2!wZIgNOEMS_<dgj>-cxCKAC%rfsPnq$oy8e?xiuLU~~67R9wmaewz
zrQGMD>CjH(!(Y{~MzLD!tH~T2zuJ7$$jy#^v~?C)qXkHAvnRiGzcQ5Pv)c8UeD;sg
zyL5eP%1j*>zncBrl1%F=&ofr)nLWuE`#9F$HomUM#MExB>wL>CvBgU|5H>rXWA@ii
zgVU0vKHtOCQEk55b7)q{7*AGaI&nRj)_Ba9r_)(pEjMWU$HWu!LX#NNux9GZ3+3C?
zn-))q5crwrmt(f75%V~;$PW5(nO<6#aq`|QzbGm7-wBe}$izV6Q?5DmWvug&s_8V+
z^z^;_6TEZpwWfSyUC*pzMAvIozvy;-Ss&&Vv&ke;Tw4y^m<vgfo`cih?J2n(k7HCS
zGo(C6cCX=_F0XY3l+SlBP-B^}aG4loPN(Uw>2kSh8Rc_}>}C-2B6z<CaU4os_pOt2
zJRjw-$N-k%bw5^%=C^xu%kbHH9wnCtvLG^EyaRa<z98vcoyB~Je%|Y&p=OPhXAvp8
za8@p}{Bk{$YIp3SOp7Y!JaW~rBZ?<)Mde~|S0V+Q`b(8X_33wDLuBo)x|#hl3vh2h
z&zt@+H3-=ZU5n*AXg94$N{*Gk(LAfQ=^n%TV{!jbeqgGe7`e-D@i*B%(e+)gEK11u
zQe?kKBIP_=(>u09!RW`(HLQO`n+ANu{RI)?w&e9f_)}G!#ypBHI=`$UwP!oRFI7j>
zYUV#mD$F;PwEFKZe6M&De7gV0Whx2LJvXWs&7m5jTQH2((YZW#lIGq*DVk*w13pUr
zwQf-P5$(O>Ozo2U@xEGU5dNJ-OLF~gy}m9R`JB;mz9gJ(>R)^pl%MKDm9nfRJJ;w8
zQ8`$ZKa_7VNZ67~!}Z*JM%hVr{qU<{XL}BotN0o<|A(ryS=n<lB0HUVwAlL&*Nb(I
zO{wG5b-VgZBDd0N)<+{xJki_t;wrsbGIg{XUVJKw+PaRn{v~>@E|;B(bWM@Ux$1l(
zj<uy~+SJx5JV;*`hCK+XbIW9?n9((>6O%tFCFt<p%G$aW5*!5wk2xEPF)0JxPy~7)
zn@U~e&r?GaA?P5{2}0X){Yd=`t4q5(Zg68;v(ru;J&XI{FZ$~{Eq&IPq{eO<J;P*g
zZ9H<k`tsETHy@wkqT6n{%V_cQop48Q^4UnK&ezPY(~5aMc=POZG31(t9m#{!1$3fv
z8tSp+3$;`^T|ocA;$|y~AZ0#n`zg6%ahH8$vy|yVkKDqWy5cPAXUgDKlmoJ33bm8C
z`^e0c7rs^BiWo2qe^+<pdHg=RjG?*ZS;yIGzYRq;_Desv<+%?_a>lblDK<%DP8~b4
z`|hp;aRO>5m7M!5Bj>7VdrQ~bdeWA4!o2i(?ngW;C5~d(rB>Q|_wddUzm~exXJO`L
zY72EZ3C}(VHPtmm=oxxkR(B;$H7qR6t;TU$)_!U{Op?x7BDUp>eYsno4|Yrvca~qv
zGxX$5iB6m%u^tn=^4en=*&8>YWVusP-Tgh{X#bvdYb4s7FeGx5zkaRwyQ+ISLF`QL
z>e;Hk>`sqG86EGdoPO$CdEWkW-yF{oSU=?m{;R!Hws`lr)QsDr&?W2o^{(*qwT_@a
zNE?nG$h)`BX)xwou!#a^%IZAMdY+K`mwF9!epg+Jo34?3M7>6yXSeBaTXRZ?pVmpu
zscX}hQwg`bw(Pmnam@E^$CUaJN#J=bl9DU=uDDz9GBcK^ht<w|WVF16=J95pg|<eP
z7P=uDBr5MKD_+yxMzqi;ve-4fp>w+vt@LUZ+PCsN3+<cJwb0Z#!b1I*jp%0a+tTWk
zJ(t$#(nU|+NFr8e<T}(H^EU1dQ}a+Sk_l4l+x~Fs1<;9Lx7&88$@R$p592A&ly9&P
zZko1uU)Z%T&8CjXS0tzAOeWY{4<}io(~#oO@--=1UE81KHB^t+<JN2O`n9agcfPuo
zH`LQ__5EFAj>KuIK=y5YzYw0oTe7u*q~DJLc4q#>B+Gg>lP(5#!0tu_kMNcaQ}5E-
zA~LmCNgQ<<9Y?&{KE=3Z7%y+<A7f@t5AsZ>3=V|Rg{0rt8<^(@t^*0WjgNMY`LR3!
zk$g{_=bqD~L~^1!-UBHS&#&euu8Y|`KXP4|$<M5cKF*w3kNTl<mGryzH7|4u<Y%1?
zu<9Z|QMRJJV>?sZe*Y@^2l(87ipH(juZ%_)h*s<jm`9>Ry%{Ceez#}`(5oBd&Z+bE
zC+R&^(;v|nj^eppeA-BQKs7m<oer654ErCxV^Mo1Uu99wU>0<<)qSn{TCdJZDf1#1
zsmI`%Qs!hBF50p6wG$<r)iKT?*KT*ReZpocZq}7j-fO<oti>P7Qi<@s6Ptdwv)nJG
z*EMHz?r%1DpO09i+w}^$>{=`2BNnZ&OcdCZe90f?=byIH^)01EOP+tXBU;z~T}%1z
z73*PI=Zf78I8S9v$8|qyZpufzvN0=2{&ZdW38&^cwZ#97@S8qR`xMic(mUUOEzdKt
z$3{-b=`*Vh=&+biTTvW#d&Se)e=P9U_UFkN=;Y(^+8lj4gHB(T6GA|<voE>lb<SVa
z&w8!n^<UY+xhdTGKLwVr-1_ox>qe2;5z*RyuX~0M?i#HANab=`4%)C0!<LU;_zF8C
cO=kM(S1vnR^L41UUdO2RgKh2*W6;L`1D%?`XaE2J

literal 0
HcmV?d00001

diff --git a/vllm/v1/core/kv_cache_manager.py b/vllm/v1/core/kv_cache_manager.py
index 6937455e7..c7437b25f 100644
--- a/vllm/v1/core/kv_cache_manager.py
+++ b/vllm/v1/core/kv_cache_manager.py
@@ -3,7 +3,7 @@
 
 from collections import defaultdict
 from dataclasses import dataclass
-from typing import Optional
+from typing import Optional, Union
 
 from vllm.distributed.kv_events import KVCacheEvent
 from vllm.logger import init_logger
@@ -14,6 +14,9 @@ from vllm.v1.core.kv_cache_utils import (BlockHash, KVCacheBlock,
 from vllm.v1.kv_cache_interface import KVCacheConfig
 from vllm.v1.metrics.stats import PrefixCacheStats
 from vllm.v1.request import Request, RequestStatus
+from unifiedcache.integration.vllm.ucm_sparse.state import get_ucm_sparse, has_ucm_sparse
+from unifiedcache.integration.vllm.ucm_sparse.base import INVALID_SLOT
+import math
 
 logger = init_logger(__name__)
 
@@ -193,6 +196,7 @@ class KVCacheManager:
         num_draft_tokens: int = 0,
         num_lookahead_tokens: int = 0,
         delay_cache_blocks: bool = False,
+        num_slots_sparsed: Union[None, int] = None
     ) -> Optional[KVCacheBlocks]:
         """Add slots for a request with new tokens to append.
 
@@ -231,6 +235,31 @@ class KVCacheManager:
         """
         if num_new_tokens == 0:
             raise ValueError("num_new_tokens must be greater than 0")
+        
+        if num_slots_sparsed != INVALID_SLOT:
+            num_blocks_need = math.ceil(num_slots_sparsed / self.block_size)
+            allocated_blocks = self.coordinator.get_blocks(request.request_id)[0]
+            returned_blocks = []
+            sparsed_blocks = []
+            for i, block in enumerate(allocated_blocks):
+                if i < num_blocks_need:
+                    sparsed_blocks.append(block)
+                else:
+                    returned_blocks.append(block)
+                self.block_pool._maybe_evict_cached_block(block)
+            self.block_pool.free_blocks(returned_blocks)
+            self.coordinator.single_type_managers[0].req_to_blocks[request.request_id] = sparsed_blocks
+            new_computed_block_list = tuple(
+                [] for _ in range(len(self.kv_cache_config.kv_cache_groups)))
+            num_blocks_to_allocate = self.coordinator.get_num_blocks_to_allocate(
+                request_id=request.request_id,
+                num_tokens=num_slots_sparsed,
+                new_computed_blocks=new_computed_block_list,
+            )
+            if num_blocks_to_allocate > self.block_pool.get_num_free_blocks():
+                return None
+            new_blocks = self.coordinator.allocate_new_blocks(request.request_id, num_slots_sparsed)
+            return KVCacheBlocks(tuple([sparsed_blocks]))
 
         if new_computed_blocks is not None:
             new_computed_block_list = new_computed_blocks.blocks
diff --git a/vllm/v1/core/sched/output.py b/vllm/v1/core/sched/output.py
index d34f39327..141d750b3 100644
--- a/vllm/v1/core/sched/output.py
+++ b/vllm/v1/core/sched/output.py
@@ -155,3 +155,6 @@ class SchedulerOutput:
 
     # KV Cache Connector metadata.
     kv_connector_metadata: Optional[KVConnectorMetadata] = None
+
+    # modified slots by sparse algorithm
+    req_sparsed_slots: dict[str, int] = None
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index fe552db74..d16785ce0 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -35,6 +35,9 @@ from vllm.v1.request import Request, RequestStatus
 from vllm.v1.spec_decode.metrics import SpecDecodingStats
 from vllm.v1.structured_output import StructuredOutputManager
 
+from unifiedcache.integration.vllm.ucm_sparse.state import ensure_ucm_sparse_initialized, get_ucm_sparse, has_ucm_sparse
+from unifiedcache.integration.vllm.ucm_sparse.base import UcmSparseBase, UcmSparseRole, INVALID_SLOT
+
 logger = init_logger(__name__)
 
 
@@ -79,12 +82,18 @@ class Scheduler(SchedulerInterface):
         # will have a corresponding KVConnector with Role=WORKER.
         # KV Connector pushes/pull of remote KVs for P/D and offloading.
         self.connector = None
+        self.ucm_sparse = None
         if self.vllm_config.kv_transfer_config is not None:
             assert len(self.kv_cache_config.kv_cache_groups) == 1, (
                 "Multiple KV cache groups are not currently supported "
                 "with KV connectors")
             self.connector = KVConnectorFactory.create_connector_v1(
                 config=self.vllm_config, role=KVConnectorRole.SCHEDULER)
+            # Initialize UCM Sparse if available
+            if "ucm_sparse_method" in vllm_config.kv_transfer_config.kv_connector_extra_config:
+                ensure_ucm_sparse_initialized(vllm_config, role=UcmSparseRole.SCHEDULER)
+                self.ucm_sparse = get_ucm_sparse()
+                logger.info("UCM Sparse initialized successfully: {}".format(self.ucm_sparse))
 
         self.kv_event_publisher = EventPublisherFactory.create(
             self.kv_events_config,
@@ -201,8 +210,13 @@ class Scheduler(SchedulerInterface):
 
         # First, schedule the RUNNING requests.
         req_index = 0
+        req_sparsed_slots: dict[str, int] = {}
         while req_index < len(self.running) and token_budget > 0:
             request = self.running[req_index]
+            num_slots_sparsed = INVALID_SLOT
+            if self.ucm_sparse:
+                num_slots_sparsed = self.ucm_sparse.estimate_num_slots_sparsed(request)
+            req_sparsed_slots.update({request.request_id: num_slots_sparsed})
 
             num_new_tokens = (request.num_tokens_with_spec -
                               request.num_computed_tokens)
@@ -250,7 +264,9 @@ class Scheduler(SchedulerInterface):
                     request,
                     num_new_tokens,
                     num_draft_tokens=num_draft_tokens,
-                    num_lookahead_tokens=self.num_lookahead_tokens)
+                    num_lookahead_tokens=self.num_lookahead_tokens,
+                    num_slots_sparsed=num_slots_sparsed
+                    )
                 if new_blocks is None:
                     # The request cannot be scheduled.
                     # Preempt the lowest-priority request.
@@ -337,6 +353,10 @@ class Scheduler(SchedulerInterface):
                     break
 
                 request = self.waiting.peek_request()
+                num_slots_sparsed = INVALID_SLOT
+                if self.ucm_sparse:
+                    num_slots_sparsed = self.ucm_sparse.estimate_num_slots_sparsed(request)
+                req_sparsed_slots.update({request.request_id: num_slots_sparsed})
 
                 # KVTransfer: skip request if still waiting for remote kvs.
                 if request.status == RequestStatus.WAITING_FOR_REMOTE_KVS:
@@ -446,6 +466,7 @@ class Scheduler(SchedulerInterface):
                     new_computed_blocks,
                     num_lookahead_tokens=self.num_lookahead_tokens,
                     delay_cache_blocks=load_kv_async,
+                    num_slots_sparsed=num_slots_sparsed
                 )
                 if new_blocks is None:
                     # The request cannot be scheduled.
@@ -559,6 +580,7 @@ class Scheduler(SchedulerInterface):
             scheduled_spec_decode_tokens=scheduled_spec_decode_tokens,
             scheduled_encoder_inputs=scheduled_encoder_inputs,
             num_common_prefix_blocks=num_common_prefix_blocks,
+            req_sparsed_slots=req_sparsed_slots,
             # finished_req_ids is an existing state in the scheduler,
             # instead of being newly scheduled in this step.
             # It contains the request IDs that are finished in between
@@ -869,6 +891,9 @@ class Scheduler(SchedulerInterface):
 
             if not stopped:
                 new_running.append(request)
+
+            if model_runner_output.finished_dumping is not None:
+                request.succeed_dumped_blocks.extend(model_runner_output.finished_dumping.get(req_id, []))
         self.running = new_running
 
         # KV Connector: update state for finished KV Transfers.
@@ -927,6 +952,8 @@ class Scheduler(SchedulerInterface):
     def add_request(self, request: Request) -> None:
         self.waiting.add_request(request)
         self.requests[request.request_id] = request
+        if self.ucm_sparse:
+            self.ucm_sparse.request_begin(request.request_id, request.prompt_token_ids)
         if self.log_stats:
             request.record_event(EngineCoreEventType.QUEUED)
 
@@ -976,7 +1003,8 @@ class Scheduler(SchedulerInterface):
 
     def _free_request(self, request: Request) -> Optional[dict[str, Any]]:
         assert request.is_finished()
-
+        if self.ucm_sparse:
+            self.ucm_sparse.request_finished_in_scheduler(request.request_id)
         delay_free_blocks, kv_xfer_params = self._connector_finished(request)
         self.encoder_cache_manager.free(request)
         request_id = request.request_id
diff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py
index f78623f57..c8388baed 100644
--- a/vllm/v1/outputs.py
+++ b/vllm/v1/outputs.py
@@ -107,6 +107,7 @@ class ModelRunnerOutput:
     # [req_ids]
     finished_sending: Optional[set[str]] = None
     finished_recving: Optional[set[str]] = None
+    finished_dumping: Optional[dict[str, list[str]]] = None
 
     # req_id -> num_nans_in_logits
     num_nans_in_logits: Optional[dict[str, int]] = None
diff --git a/vllm/v1/request.py b/vllm/v1/request.py
index 9b96f4599..e70d1695b 100644
--- a/vllm/v1/request.py
+++ b/vllm/v1/request.py
@@ -103,6 +103,8 @@ class Request:
         # The number of tokens with prefix cache hits.
         self.num_cached_tokens = -1
 
+        self.succeed_dumped_blocks: list[str] = []
+
         # The number of NaNs in logits. A value greater than 0
         # indicates that the output is corrupted
         self.num_nans_in_logits = 0
diff --git a/vllm/v1/worker/block_table.py b/vllm/v1/worker/block_table.py
index 8f4e8d64c..d5be44680 100644
--- a/vllm/v1/worker/block_table.py
+++ b/vllm/v1/worker/block_table.py
@@ -60,6 +60,15 @@ class BlockTable:
         start = self.num_blocks_per_row[row_idx]
         self.num_blocks_per_row[row_idx] += num_blocks
         self.block_table_np[row_idx, start:start + num_blocks] = block_ids
+    
+    def reset_row(
+            self,
+            row_idx: int,
+    ) -> None:
+        self.num_blocks_per_row[row_idx] = 0
+        self.block_table[row_idx].fill_(0)
+        self.block_table_cpu[row_idx].fill_(0)
+        self.block_table_np[row_idx].fill(0)
 
     def add_row(self, block_ids: list[int], row_idx: int) -> None:
         self.num_blocks_per_row[row_idx] = 0
@@ -117,6 +126,10 @@ class MultiGroupBlockTable:
         for i, block_table in enumerate(self.block_tables):
             block_table.append_row(block_ids[i], row_idx)
 
+    def reset_row(self, row_idx: int) -> None:
+        for i, block_table in enumerate(self.block_tables):
+            block_table.reset_row(row_idx)
+
     def add_row(self, block_ids: tuple[list[int], ...], row_idx: int) -> None:
         for i, block_table in enumerate(self.block_tables):
             block_table.add_row(block_ids[i], row_idx)
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 5a26e88db..14278bb6a 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -1378,7 +1378,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 inputs_embeds=inputs_embeds,
             )
 
-            self.maybe_wait_for_kv_save()
+            finished_dumping = self.maybe_wait_for_kv_save()
             finished_sending, finished_recving = (
                 self.get_finished_kv_transfers(scheduler_output))
 
@@ -1563,6 +1563,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             finished_sending=finished_sending,
             finished_recving=finished_recving,
             num_nans_in_logits=num_nans_in_logits,
+            finished_dumping=finished_dumping
         )
 
     def propose_draft_token_ids(
@@ -1719,9 +1720,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             kv_connector.start_load_kv(get_forward_context())
 
     @staticmethod
-    def maybe_wait_for_kv_save() -> None:
+    def maybe_wait_for_kv_save() -> Optional[dict[str, list[str]]]:
         if has_kv_transfer_group():
-            get_kv_transfer_group().wait_for_save()
+            return get_kv_transfer_group().wait_for_save()
 
     @staticmethod
     def get_finished_kv_transfers(
-- 
2.50.1.windows.1

