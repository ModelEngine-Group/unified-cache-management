{"input": "What was the Buckeyes' record in their first game of the season?", "context": "The 1951 Ohio State Buckeyes baseball team represented the Ohio State University in the 1951 NCAA baseball season. The head coach was Marty Karow, serving his 1st year.\n\nThe Buckeyes lost in the College World Series, defeated by the Texas A&M Aggies.\n\nRoster\n\nSchedule \n\n! style=\"\" | Regular Season\n|- valign=\"top\" \n\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 1 || March 16 || at  || Unknown • San Antonio, Texas || 15–3 || 1–0 || 0–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 2 || March 17 || at B. A. M. C. || Unknown • San Antonio, Texas || 7–8 || 1–1 || 0–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 3 || March 19 || at  || Clark Field • Austin, Texas || 0–8 || 1–2 || 0–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 4 || March 20 || at Texas || Clark Field • Austin, Texas || 3–4 || 1–3 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 5 || March 21 || at  || Unknown • Houston, Texas || 14–6 || 2–3 || 0–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 6 || March 22 || at Rice || Unknown • Houston, Texas || 2–3 || 2–4 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 7 || March 23 || at  || Unknown • Fort Worth, Texas || 4–2 || 3–4 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 8 || March 24 || at TCU || Unknown • Fort Worth, Texas || 7–3 || 4–4 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 9 || March 24 || at  || Unknown • St. Louis, Missouri || 10–4 || 5–4 || 0–0\n|-\n\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 10 || April 6 ||  || Varsity Diamond • Columbus, Ohio || 2–0 || 6–4 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 11 || April 7 ||  || Varsity Diamond • Columbus, Ohio || 15–1 || 7–4 || 0–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 12 || April 14 ||  || Varsity Diamond • Columbus, Ohio || 0–1 || 7–5 || 0–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 13 || April 20 ||  || Varsity Diamond • Columbus, Ohio || 10–9 || 8–5 || 1–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 14 || April 21 || Minnesota || Varsity Diamond • Columbus, Ohio || 7–0 || 9–5 || 2–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 15 || April 24 || at  || Unknown • Oxford, Ohio || 3–4 || 9–6 || 2–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 16 || April 27 || at  || Hyames Field • Kalamazoo, Michigan || 2–3 || 9–7 || 2–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 17 || April 28 || at Western Michigan || Hyames Field • Kalamazoo, Michigan || 5–7 || 9–8 || 2–0\n|-\n\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 18 || May 1 || at  || Unknown • Athens, Ohio || 7–6 || 10–8 || 2–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 19 || May 4 ||  || Varsity Diamond • Columbus, Ohio || 12–6 || 11–8 || 3–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 20 || May 5 || Purdue || Varsity Diamond • Columbus, Ohio || 14–4 || 12–8 || 4–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 21 || May 8 ||  || Varsity Diamond • Columbus, Ohio || 6–8 || 12–9 || 4–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 22 || May 9 || at Dayton || Unknown • Dayton, Ohio || 11–2 || 13–9 || 4–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 23 || May 12 ||  || Varsity Diamond • Columbus, Ohio || 6–5 || 14–9 || 5–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 24 || May 12 || Indiana || Varsity Diamond • Columbus, Ohio || 5–2 || 15–9 || 6–0\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 25 || May 15 || Ohio || Varsity Diamond • Columbus, Ohio || 6–0 || 16–9 || 6–0\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 26 || May 18 || at  || Northwestern Park • Evanston, Illinois || 1–3 || 16–10 || 6–1\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 27 || May 19 || at Northwestern || Northwestern Park • Evanston, Illinois || 10–3 || 17–10 || 7–1\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 28 || May 22 || at Cincinnati || Carson Field • Cincinnati, Ohio || 8–4 || 18–10 || 7–1\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 29 || May 25 ||  || Varsity Diamond • Columbus, Ohio || 4–1 || 19–10 || 8–1\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 30 || May 25 || Michigan || Varsity Diamond • Columbus, Ohio || 3–6 || 19–11 || 8–2\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 31 || May 30 || Miami (OH) || Varsity Diamond • Columbus, Ohio || 3–4 || 19–12 || 8–2\n|-\n\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 32 || June 1 || at  || Old College Field • East Lansing, Michigan || 8–0 || 20–12 || 9–2\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 33 || June 2 || at Michigan State || Old College Field • East Lansing, Michigan || 9–8 || 21–12 || 10–2\n|-\n\n|-\n|-\n! style=\"\" | Postseason\n|- valign=\"top\"\n\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 34 || June 8 || Western Michigan || Varsity Diamond • Columbus, Ohio || 1–0 || 22–12 || 10–2\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 35 || June 8 || Western Michigan || Varsity Diamond • Columbus, Ohio || 2–4 || 22–13 || 10–2\n|- align=\"center\" bgcolor=\"#ccffcc\"\n| 36 || June 9 || Western Michigan || Varsity Diamond • Columbus, Ohio || 3–2 || 23–13 || 10–2\n|-\n\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 37 || June 13 || Oklahoma || Omaha Municipal Stadium • Omaha, Nebraska || 8–9 || 23–14 || 10–2\n|- align=\"center\" bgcolor=\"#ffcccc\"\n| 38 || June 13 || Texas A&M || Omaha Municipal Stadium • Omaha, Nebraska || 2–3 || 23–15 || 10–2\n|-\n\nAwards and honors \nDick Hauck\n First Team All-Big Ten\n\nStewart Hein\n First Team All-Big Ten\n\nReferences \n\nOhio State Buckeyes baseball seasons\nOhio State Buckeyes baseball\nBig Ten Conference baseball champion seasons\nOhio State\nCollege World Series seasons", "answers": ["They won their first game with a score of 15-3."], "length": 972, "dataset": "multifieldqa_en", "language": "en", "all_classes": null, "_id": "ed61bdde19a3446389e989c06ab4209f464f9484d42dbd1c"}
{"input": "How many brother does Njoroge have?", "context": "Weep Not, Child is a 1964 novel by Kenyan author Ngũgĩ wa Thiong'o. It was his first novel, published in 1964 under the name James Ngugi. It was among the African Writers Series. It was the first English language|English novel to be published by an East African. Thiong'o's works deal with the relationship between Africans and white settlers in colonial Kenya, and are heavily critical of colonial rule. Specifically, Weep Not, Child deals with the Mau Mau Uprising, and \"the bewildering dispossession of an entire people from their ancestral land.\" Ngũgĩ wrote the novel while he was a student at Makerere University.\n\nThe book is divided into two parts and eighteen chapters. Part one deals mostly with the education of Njoroge, while part two deals with the rising Mau Mau movement.\n\nPlot summary\n\nNjoroge, a little boy, is urged to attend school by his mother. He is the first one of his family able to go to school. His family lives on the land of Jacobo, an African made rich by his dealings with white settlers, namely Mr. Howlands, the most powerful land owner in the area. Njoroge's brother Kamau works as an apprentice to a carpenter, while Boro, the eldest living son, is troubled by his experiences while in forced service during World War II, including witnessing the death of his elder brother. Ngotho, Njoroge's father and a respected man in the surrounding area, tends Mr. Howlands' crops, but is motivated by his passion to preserve his ancestral land, rather than for any compensation or loyalty.\n\nOne day, black workers call for a strike to obtain higher wages. Ngotho is ambivalent about participating in the strike because he fears he will lose his job. However, he decides to go to the gathering, even though his two wives do not agree. At the demonstration, there are calls for higher wages. Suddenly, the white police inspector brings Jacobo to the gathering to pacify the native people. Jacobo tries to put an end to the strike. Ngotho attacks Jacobo, and the result is a riot where two people are killed. Jacobo survives and swears revenge. Ngotho loses his job and Njoroge’s family is forced to move. Njoroge’s brothers fund his education and seem to lose respect for their father.\n\nMwihaki, Jacobo's daughter and Njoroge's best friend, enters a girls' only boarding school, leaving Njoroge relatively alone. He reflects upon her leaving, and realizes that he was embarrassed by his father's actions towards Jacobo. For this reason, Njoroge is not upset by her exit and their separation. Njoroge switches to another school.\n\nFor a time, everyone's attention is focused on the upcoming trial of Jomo Kenyatta – a revered leader of the movement. Many blacks think that he is going to bring forth Kenya’s independence. But Jomo loses the trial and is imprisoned. This results in further protests and greater suppression of the black population.\n\nJacobo and a white landowner, Mr. Howlands, fight against the rising activities of the Mau Mau, an organization striving for Kenyan economic, political, and cultural independence. Jacobo accuses Ngotho of being the leader of the Mau Mau and tries to imprison the whole family. Meanwhile, the situation in the country is deteriorating. Six black men are taken out of their houses and executed in the woods.\n\nOne day Njoroge meets Mwihaki again, who has returned from boarding school. Although Njoroge had planned to avoid her due to the conflict between their fathers, their friendship is unaffected. Njoroge passes an important exam that allows him to advance to High School. His village is proud of him, and collects money to pay Njoroge's High School tuition.\n\nSeveral months later, Jacobo is murdered in his office by a member of the Mau Mau. Mr. Howlands has Njoroge removed from school for questioning. Both father and son are brutally beaten before release and Ngotho is left barely alive. Although there doesn't seem to be a connection between Njoroge's family and the murder, it is eventually revealed that Njoroge's brothers are behind the assassination, and that Boro is the real leader of the Mau Mau. Ngotho soon dies from his injuries and Njoroge finds out that his father was protecting his brothers. Kamau has been imprisoned for life. Only Njoroge and his two mothers remain free, and Njoroge is left as the sole provider of his two mothers. Njoroge fears that he cannot make ends meet; he gives up hope of continuing in school and loses faith in God.\n\nNjoroge asks Mwihaki's for support, but she is angry because of her father’s death. When he finally pledges his love to her, she refuses to leave with him, realizing her obligation to Kenya and her mother. Njoroge decides to leave town and makes an attempt at suicide; however, he fails when his mothers find him before he is able to hang himself. The novel closes with Njoroge feeling hopeless, and ashamed of cowardice.\n\nCharacters in Weep Not, Child\n Njoroge: the main character of the book whose main goal throughout the book is to become as educated as possible.\n Ngotho: Njoroge's father. He works for Mr.Howlands and is respected by him until he attacks Jacobo at a workers strike. He is fired and the family is forced to move to another section of the country. Over the course of the book his position as the central power of the family weakened, to the point where his self-realization that he has spent his whole life waiting for the prophecy (that proclaims the blacks will be returned their land) to come true rather than fighting for Kenyan independence, leads to his depression.\n Nyokabi and Njeri: the two wives of Ngotho. Njeri is Ngotho's first wife, and mother of Boro, Kamau, and Kori. Nyokabi is his second wife, and the mother of Njoroge and Mwangi.\n Njoroge has four brothers: Boro, Kamau, Kori and Mwangi (who is Njoroge's only full brother, who died in World War II).\n Boro: Son of Njeri who fights for the Allies in World War II. Upon returning his anger against the colonial government is compounded by their confiscation of the his land. Boro's anger and position as eldest son leads him to question and ridicule Ngotho, which eventually defeats their father's will (upon realizing his life was wasted waiting and not acting). It is eventually revealed that Boro is the leader of the Mau Mau (earlier alluded to as \"entering politics\") and murders Mr.Howlands. He is caught by police immediately after and is scheduled to be executed by the book's end. It is highly likely that it is also Boro who kills Jacobo.\n Mwihaki: Njoroge's best friend (and later develops into his love interest). Daughter of Jacobo. When it is revealed that his family killed Jacobo (most likely Boro), Mwihaki distances herself from Njoroge, asking for time to mourn her father and care for her mother.\n Jacobo: Mwihaki's father and an important landowner. Chief of the village.\n Mr. Howlands: A white settler who emigrated to colonial Kenya and now owns a farm made up of land that originally belonged to Ngotho's ancestors. Has three children: Peter who died in World War II before the book's beginning, a daughter who becomes a missionary, and Stephen who met Njoroge while the two were in high school.\n\nThemes and motifs\nWeep Not, Child integrates Gikuyu mythology and the ideology of nationalism that serves as catalyst for much of the novel's action. The novel explores the negative aspects of colonial rule over Kenya. Njoroge's aspiration to attend university is frustrated by both the violence of the Mau Mau rebels and the violent response of the colonial government. This disappointment leads to his alienation from his family and ultimately his suicide attempt.\n\nThe novel also ponders the role of saviours and salvation. The author notes in his The River Between: \"Salvation shall come from the hills. From the blood that flows in me, I say from the same tree, a son shall rise. And his duty shall be to lead and save the people.\" Jomo Kenyatta, the first prime minister of Kenya, is immortalised in Weep Not, Child. The author says, \"Jomo had been his (Ngotho's) hope. Ngotho had come to think that it was Jomo who would drive away the white man. To him, Jomo stood for custom and traditions purified by grace of learning and much travel.\" Njoroge comes to view Jomo as a messiah who will win the struggle against the colonial government.", "answers": ["Four."], "length": 1414, "dataset": "multifieldqa_en", "language": "en", "all_classes": null, "_id": "69e69439e349a539ca4cff96ae35aa8499ca61886801d488"}
{"input": "How do they obtain psychological dimensions of people?", "context": "Introduction\nBlogging gained momentum in 1999 and became especially popular after the launch of freely available, hosted platforms such as blogger.com or livejournal.com. Blogging has progressively been used by individuals to share news, ideas, and information, but it has also developed a mainstream role to the extent that it is being used by political consultants and news services as a tool for outreach and opinion forming as well as by businesses as a marketing tool to promote products and services BIBREF0 .\nFor this paper, we compiled a very large geolocated collection of blogs, written by individuals located in the U.S., with the purpose of creating insightful mappings of the blogging community. In particular, during May-July 2015, we gathered the profile information for all the users that have self-reported their location in the U.S., along with a number of posts for all their associated blogs. We utilize this blog collection to generate maps of the U.S. that reflect user demographics, language use, and distributions of psycholinguistic and semantic word classes. We believe that these maps can provide valuable insights and partial verification of previous claims in support of research in linguistic geography BIBREF1 , regional personality BIBREF2 , and language analysis BIBREF3 , BIBREF4 , as well as psychology and its relation to human geography BIBREF5 .\nData Collection\nOur premise is that we can generate informative maps using geolocated information available on social media; therefore, we guide the blog collection process with the constraint that we only accept blogs that have specific location information. Moreover, we aim to find blogs belonging to writers from all 50 U.S. states, which will allow us to build U.S. maps for various dimensions of interest.\nWe first started by collecting a set of profiles of bloggers that met our location specifications by searching individual states on the profile finder on http://www.blogger.com. Starting with this list, we can locate the profile page for a user, and subsequently extract additional information, which includes fields such as name, email, occupation, industry, and so forth. It is important to note that the profile finder only identifies users that have an exact match to the location specified in the query; we thus built and ran queries that used both state abbreviations (e.g., TX, AL), as well as the states' full names (e.g., Texas, Alabama).\nAfter completing all the processing steps, we identified 197,527 bloggers with state location information. For each of these bloggers, we found their blogs (note that a blogger can have multiple blogs), for a total of 335,698 blogs. For each of these blogs, we downloaded the 21 most recent blog postings, which were cleaned of HTML tags and tokenized, resulting in a collection of 4,600,465 blog posts.\nMaps from Blogs\nOur dataset provides mappings between location, profile information, and language use, which we can leverage to generate maps that reflect demographic, linguistic, and psycholinguistic properties of the population represented in the dataset.\nPeople Maps\nThe first map we generate depicts the distribution of the bloggers in our dataset across the U.S. Figure FIGREF1 shows the density of users in our dataset in each of the 50 states. For instance, the densest state was found to be California with 11,701 users. The second densest is Texas, with 9,252 users, followed by New York, with 9,136. The state with the fewest bloggers is Delaware with 1,217 users. Not surprisingly, this distribution correlates well with the population of these states, with a Spearman's rank correlation INLINEFORM0 of 0.91 and a p-value INLINEFORM1 0.0001, and is very similar to the one reported in Lin and Halavais Lin04.\nFigure FIGREF1 also shows the cities mentioned most often in our dataset. In particular, it illustrates all 227 cities that have at least 100 bloggers. The bigger the dot on the map, the larger the number of users found in that city. The five top blogger-dense cities, in order, are: Chicago, New York, Portland, Seattle, and Atlanta.\nWe also generate two maps that delineate the gender distribution in the dataset. Overall, the blogging world seems to be dominated by females: out of 153,209 users who self-reported their gender, only 52,725 are men and 100,484 are women. Figures FIGREF1 and FIGREF1 show the percentage of male and female bloggers in each of the 50 states. As seen in this figure, there are more than the average number of male bloggers in states such as California and New York, whereas Utah and Idaho have a higher percentage of women bloggers.\nAnother profile element that can lead to interesting maps is the Industry field BIBREF6 . Using this field, we created different maps that plot the geographical distribution of industries across the country. As an example, Figure FIGREF2 shows the percentage of the users in each state working in the automotive and tourism industries respectively.\nLinguistic Maps\nAnother use of the information found in our dataset is to build linguistic maps, which reflect the geographic lexical variation across the 50 states BIBREF7 . We generate maps that represent the relative frequency by which a word occurs in the different states. Figure FIGREF3 shows sample maps created for two different words. The figure shows the map generated for one location specific word, Maui, which unsurprisingly is found predominantly in Hawaii, and a map for a more common word, lake, which has a high occurrence rate in Minnesota (Land of 10,000 Lakes) and Utah (home of the Great Salt Lake). Our demo described in Section SECREF4 , can also be used to generate maps for function words, which can be very telling regarding people's personality BIBREF8 .\nPsycholinguistic and Semantic Maps\nLIWC. In addition to individual words, we can also create maps for word categories that reflect a certain psycholinguistic or semantic property. Several lexical resources, such as Roget or Linguistic Inquiry and Word Count BIBREF9 , group words into categories. Examples of such categories are Money, which includes words such as remuneration, dollar, and payment; or Positive feelings with words such as happy, cheerful, and celebration. Using the distribution of the individual words in a category, we can compile distributions for the entire category, and therefore generate maps for these word categories. For instance, figure FIGREF8 shows the maps created for two categories: Positive Feelings and Money. The maps are not surprising, and interestingly they also reflect an inverse correlation between Money and Positive Feelings .\nValues. We also measure the usage of words related to people's core values as reported by Boyd et al. boyd2015. The sets of words, or themes, were excavated using the Meaning Extraction Method (MEM) BIBREF10 . MEM is a topic modeling approach applied to a corpus of texts created by hundreds of survey respondents from the U.S. who were asked to freely write about their personal values. To illustrate, Figure FIGREF9 shows the geographical distributions of two of these value themes: Religion and Hard Work. Southeastern states often considered as the nation's “Bible Belt” BIBREF11 were found to have generally higher usage of Religion words such as God, bible, and church. Another broad trend was that western-central states (e.g., Wyoming, Nebraska, Iowa) commonly blogged about Hard Work, using words such as hard, work, and job more often than bloggers in other regions.\nWeb Demonstration\nA prototype, interactive charting demo is available at http://lit.eecs.umich.edu/~geoliwc/. In addition to drawing maps of the geographical distributions on the different LIWC categories, the tool can report the three most and least correlated LIWC categories in the U.S. and compare the distributions of any two categories.\nConclusions\nIn this paper, we showed how we can effectively leverage a prodigious blog dataset. Not only does the dataset bring out the extensive linguistic content reflected in the blog posts, but also includes location information and rich metadata. These data allow for the generation of maps that reflect the demographics of the population, variations in language use, and differences in psycholinguistic and semantic categories. These mappings can be valuable to both psychologists and linguists, as well as lexicographers. A prototype demo has been made available together with the code used to collect our dataset.\nAcknowledgments\nThis material is based in part upon work supported by the National Science Foundation (#1344257) and by the John Templeton Foundation (#48503). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or the John Templeton Foundation. We would like to thank our colleagues Hengjing Wang, Jiatao Fan, Xinghai Zhang, and Po-Jung Huang who provided technical help with the implementation of the demo.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "length": 1440, "dataset": "qasper", "language": "en", "all_classes": null, "_id": "43ffd7775c3b4a541e227c120bffcc7c7b31fb184ba94d69"}
{"input": "Were any of these tasks evaluated in any previous work?", "context": "Introduction\nThe recently introduced BERT model BIBREF0 exhibits strong performance on several language understanding benchmarks. To what extent does it capture syntax-sensitive structures?\nRecent work examines the extent to which RNN-based models capture syntax-sensitive phenomena that are traditionally taken as evidence for the existence in hierarchical structure. In particular, in BIBREF1 we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences. BIBREF2 also consider subject-verb agreement, but in a “colorless green ideas” setting in which content words in naturally occurring sentences are replaced with random words with the same part-of-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues. BIBREF3 consider a wider range of syntactic phenomena (subject-verb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting.\nThe BERT model is based on the “Transformer” architecture BIBREF4 , which—in contrast to RNNs—relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding. This reliance on attention may lead one to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence. Indeed, BIBREF5 finds that transformer-based models perform worse than LSTM models on the BIBREF1 agreement prediction dataset. In contrast, BIBREF6 find that self-attention performs on par with LSTM for syntax sensitive dependencies in the context of machine-translation, and performance on syntactic tasks is correlated with the number of attention heads in multi-head attention.\nI adapt the evaluation protocol and stimuli of BIBREF1 , BIBREF2 and BIBREF3 to the bidirectional setting required by BERT, and evaluate the pre-trained BERT models (both the Large and the Base models). Surprisingly (at least to me), the out-of-the-box models (without any task-specific fine-tuning) perform very well on all the syntactic tasks.\nMethodology\nI use the stimuli provided by BIBREF1 , BIBREF2 , BIBREF3 , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model. This requires discarding some of the stimuli, as described below. Thus, the numbers are not strictly comparable to those reported in previous work.\nPrevious setups\nAll three previous work use uni-directional language-model-like models.\nBIBREF1 start with existing sentences from wikipedia that contain a present-tense verb. They feed each sentence word by word into an LSTM, stop right before the focus verb, and ask the model to predict a binary plural/singular decision (supervised setup) or compare the probability assigned by a pre-trained language model (LM) to the plural vs singular forms of the verb (LM setup). The evaluation is then performed on sentences with “agreement attractors” in which at there is at least one noun between the verb and its subject, and all of the nouns between the verb and subject are of the opposite number from the subject.\nBIBREF2 also start with existing sentences. However, in order to control for the possibillity of the model learning to rely on “semantic” selectional-preferences cues rather than syntactic ones, they replace each content word with random words from the same part-of-speech and inflection. This results in “coloreless green ideas” nonce sentences. The evaluation is then performed similarly to the LM setup of BIBREF1 : the sentence is fed into a pre-traiend LSTM LM up to the focus verb, and the model is considered correct if the probability assigned to the correct inflection of the original verb form given the prefix is larger than that assigned to the incorrect inflection.\nBIBREF3 focus on manually constructed and controlled stimuli, that also emphasizes linguistic structure over selectional preferences. They construct minimal pairs of grammatical and ungrammatical sentences, feed each one in its entirety into a pre-trained LSTM-LM, and compare the perplexity assigned by the model to the grammatical and ungrammatical sentences. The model is “correct” if it assigns the grammatical sentence a higher probability than to the ungrammatical one. Since the minimal pairs for most phenomena differ only in a single word (the focus verb), this scoring is very similar to the one used in the two previous works. However, it does consider the continuation of the sentence after the focus verb, and also allows for assessing phenomena that require change into two or more words (like negative polarity items).\nAdaptation to the BERT model\nIn contrast to these works, the BERT model is bi-directional: it is trained to predict the identity of masked words based on both the prefix and suffix surrounding these words. I adapt the uni-directional setup by feeding into BERT the complete sentence, while masking out the single focus verb. I then ask BERT for its word predictions for the masked position, and compare the score assigned to the original correct verb to the score assigned to the incorrect one.\nFor example, for the sentence:\na 2002 systemic review of herbal products found that several herbs , including peppermint and caraway , have anti-dyspeptic effects for non-ulcer dyspepsia with “ encouraging safety profiles ” . (from BIBREF1 )\nI feed into BERT:\n[CLS] a 2002 systemic review of herbal products found that several herbs , including peppermint and caraway , [MASK] anti-dyspeptic effects for non-ulcer dyspepsia with “ encouraging safety profiles ” . and look for the score assigned to the words have and has at the masked position.\nSimilarly, for the pair\nthe game that the guard hates is bad .\nthe game that the guard hates are bad .\n(from BIBREF3 ), I feed into BERT:\n[CLS] the game that the guard hates [MASK] bad .\nand compare the scores predicted for is and are.\nThis differs from BIBREF1 and BIBREF2 by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from BIBREF3 by conditioning the focus verb on bidirectional context.\nI use the PyTorch implementation of BERT, with the pre-trained models supplied by Google. I experiment with the bert-large-uncased and bert-base-uncased models.\nThe bi-directional setup precludes using using the NPI stimuli of BIBREF3 , in which the minimal pair differs in two words position, which I discard from the evaluation. I also discard the agreement cases involving the verbs is or are in BIBREF1 and in BIBREF2 , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb. This is not an issue in the manually constructed BIBREF3 stimuli due to the patterns they chose.\nFinally, I discard stimuli in which the focus verb or its plural/singular inflection does not appear as a single word in the BERT word-piece-based vocabulary (and hence cannot be predicted by the model). This include discarding BIBREF3 stimuli involving the words swims or admires, resulting in 23,368 discarded pairs (out of 152,300). I similarly discard 680 sentences from BIBREF1 where the focus verb or its inflection were one of 108 out-of-vocabulary tokens, and 28 sentence-pairs (8 tokens) from BIBREF2 .\nThe BERT results are not directly comparable to the numbers reported in previous work. Beyond the differences due to bidirectionality and the discarded stimuli, the BERT models are also trained on a different and larger corpus (covering both wikipedia and books).\nCode is available at https://github.com/yoavg/bert-syntax.\nResults\nTables 1 , 2 and 3 show the results. All cases exhibit high scores—in the vast majority of the cases substantially higher than reported in previous work. As discussed above, the results are not directly comparable to previous work: the BERT models are trained on different (and larger) data, are allowed to access the suffix of the sentence in addition to its prefix, and are evaluated on somewhat different data due to discarding OOV items. Still, taken together, the high performance numbers indicate that the purely attention-based BERT models are likely capable of capturing the same kind of syntactic regularities that LSTM-based models are capable of capturing, at least as well as the LSTM models and probably better.\nAnother noticeable and interesting trend is that larger is not necessarily better: the BERT-Base model outperforms the BERT-Large model on many of the syntactic conditions.\nDiscussion\nThe BERT models perform remarkably well on all the syntactic test cases. I expected the attention-based mechanism to fail on these (compared to the LSTM-based models), and am surprised by these results. The BIBREF2 and BIBREF3 conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place. Exploring the extent to which deep purely-attention-based architectures such as BERT are capable of capturing hierarchy-sensitive and syntactic dependencies—as well as the mechanisms by which this is achieved—is a fascinating area for future research.", "answers": ["Yes", "Yes"], "length": 1464, "dataset": "qasper", "language": "en", "all_classes": null, "_id": "6a78dbe4f8e30d35c13ea1f80e52df286f9e1f664f9c3d98"}
{"input": "Do the answered questions measure for the usefulness of the answer?", "context": "Introduction\nFrom a group of small users at the time of its inception in 2009, Quora has evolved in the last few years into one of the largest community driven Q&A sites with diverse user communities. With the help of efficient content moderation/review policies and active in-house review team, efficient Quora bots, this site has emerged into one of the largest and reliable sources of Q&A on the Internet. On Quora, users can post questions, follow questions, share questions, tag them with relevant topics, follow topics, follow users apart from answering, commenting, upvoting/downvoting etc. The integrated social structure at the backbone of it and the topical organization of its rich content have made Quora unique with respect to other Q&A sites like Stack Overflow, Yahoo! Answers etc. and these are some of the prime reasons behind its popularity in recent times. Quality question posting and getting them answered are the key objectives of any Q&A site. In this study we focus on the answerability of questions on Quora, i.e., whether a posted question shall eventually get answered. In Quora, the questions with no answers are referred to as “open questions”. These open questions need to be studied separately to understand the reason behind their not being answered or to be precise, are there any characteristic differences between `open' questions and the answered ones. For example, the question “What are the most promising advances in the treatment of traumatic brain injuries?” was posted on Quora on 23rd June, 2011 and got its first answer after almost 2 years on 22nd April, 2013. The reason that this question remained open so long might be the hardness of answering it and the lack of visibility and experts in the domain. Therefore, it is important to identify the open questions and take measures based on the types - poor quality questions can be removed from Quora and the good quality questions can be promoted so that they get more visibility and are eventually routed to topical experts for better answers.\nCharacterization of the questions based on question quality requires expert human interventions often judging if a question would remain open based on factors like if it is subjective, controversial, open-ended, vague/imprecise, ill-formed, off-topic, ambiguous, uninteresting etc. Collecting judgment data for thousands of question posts is a very expensive process. Therefore, such an experiment can be done only for a small set of questions and it would be practically impossible to scale it up for the entire collection of posts on the Q&A site. In this work, we show that appropriate quantification of various linguistic activities can naturally correspond to many of the judgment factors mentioned above (see table 2 for a collection of examples). These quantities encoding such linguistic activities can be easily measured for each question post and thus helps us to have an alternative mechanism to characterize the answerability on the Q&A site.\nThere are several research works done in Q&A focusing on content of posts. BIBREF0 exploit community feedback to identify high quality content on Yahoo! Answers. BIBREF1 use textual features to predict answer quality on Yahoo! Answers. BIBREF2 , investigate predictors of answer quality through a comparative, controlled field study of user responses. BIBREF3 study the problem of how long questions remain unanswered. BIBREF4 propose a prediction model on how many answers a question shall receive. BIBREF5 analyze and predict unanswered questions on Yahoo Answers. BIBREF6 study question quality in Yahoo! Answers.\nDataset description\nWe obtained our Quora dataset BIBREF7 through web-based crawls between June 2014 to August 2014. This crawling exercise has resulted in the accumulation of a massive Q&A dataset spanning over a period of over four years starting from January 2010 to May 2014. We initiated crawling with 100 questions randomly selected from different topics so that different genre of questions can be covered. The crawling of the questions follow a BFS pattern through the related question links. We obtained 822,040 unique questions across 80,253 different topics with a total of 1,833,125 answers to these questions. For each question, we separately crawl their revision logs that contain different types of edit information for the question and the activity log of the question asker.\nLinguistic activities on Quora\nIn this section, we identify various linguistic activities on Quora and propose quantifications of the language usage patterns in this Q&A site. In particular, we show that there exists significant differences in the linguistic structure of the open and the answered questions. Note that most of the measures that we define are simple, intuitive and can be easily obtained automatically from the data (without manual intervention). Therefore the framework is practical, inexpensive and highly scalable.\nContent of a question text is important to attract people and make them engage more toward it. The linguistic structure (i.e., the usage of POS tags, the use of Out-of-Vocabulary words, character usage etc.) one adopts are key factors for answerability of questions. We shall discuss the linguistic structure that often represents the writing style of a question asker.\nIn fig 1 (a), we observe that askers of open questions generally use more no. of words compared to answered questions. To understand the nature of words (standard English words or chat-like words frequently used in social media) used in the text, we compare the words with GNU Aspell dictionary to see whether they are present in the dictionary or not. We observe that both open questions and answered questions follow similar distribution (see fig 1 (b)). Part-of-Speech (POS) tags are indicators of grammatical aspects of texts. To observe how the Part-of-Speech tags are distributed in the question texts, we define a diversity metric. We use the standard CMU POS tagger BIBREF8 for identifying the POS tags of the constituent words in the question. We define the POS tag diversity (POSDiv) of a question $q_i$ as follows: $POSDiv(q_i) = -\\sum _{j \\in pos_{set}}p_j\\times \\log (p_j)$ where $p_j$ is the probability of the $j^{th}$ POS in the set of POS tags. Fig 1 (c) shows that the answered questions have lower POS tag diversity compared to open questions. Question texts undergo several edits so that their readability and the engagement toward them are enhanced. It is interesting to identify how far such edits can make the question different from the original version of it. To capture this phenomena, we have adopted ROUGE-LCS recall BIBREF9 from the domain of text summarization. Higher the recall value, lesser are the changes in the question text. From fig 1 (d), we observe that open questions tend to have higher recall compared to the answered ones which suggests that they have not gone through much of text editing thus allowing for almost no scope of readability enhancement.\nPsycholinguistic analysis:\nThe way an individual talks or writes, give us clue to his/her linguistic, emotional, and cognitive states. A question asker's linguistic, emotional, cognitive states are also revealed through the language he/she uses in the question text. In order to capture such psycholinguistic aspects of the asker, we use Linguistic Inquiry and Word Count (LIWC) BIBREF10 that analyzes various emotional, cognitive, and structural components present in individuals' written texts. LIWC takes a text document as input and outputs a score for the input for each of the LIWC categories such as linguistic (part-of-speech of the words, function words etc.) and psychological categories (social, anger, positive emotion, negative emotion, sadness etc.) based on the writing style and psychometric properties of the document. In table 1 , we perform a comparative analysis of the asker's psycholinguistic state while asking an open question and an answered question.\nAskers of open questions use more function words, impersonal pronouns, articles on an average whereas asker of answered questions use more personal pronouns, conjunctions and adverbs to describe their questions. Essentially, open questions lack content words compared to answered questions which, in turn, affects the readability of the question. As far as the psychological aspects are concerned, answered question askers tend to use more social, family, human related words on average compared to an open question asker. The open question askers express more positive emotions whereas the answered question asker tend to express more negative emotions in their texts. Also, answered question askers are more emotionally involved and their questions reveal higher usage of anger, sadness, anxiety related words compared to that of open questions. Open questions, on the other hand, contains more sexual, body, health related words which might be reasons why they do not attract answers.\nIn table 2 , we show a collection of examples of open questions to illustrate that many of the above quantities based on the linguistic activities described in this section naturally correspond to the factors that human judges consider responsible for a question remaining unanswered. This is one of the prime reasons why these quantities qualify as appropriate indicators of answerability.\nPrediction model\nIn this section, we describe the prediction framework in detail. Our goal is to predict whether a given question after a time period $t$ will be answered or not.\nLinguistic styles of the question asker\nThe content and way of posing a question is important to attract answers. We have observed in the previous section that these linguistic as well as psycholinguistic aspects of the question asker are discriminatory factors. For the prediction, we use the following features:", "answers": ["No"], "length": 1561, "dataset": "qasper", "language": "en", "all_classes": null, "_id": "37f06d3a413beef1661730ae1049e7adccd6308032ae1fc5"}
