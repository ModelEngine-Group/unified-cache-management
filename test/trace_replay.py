# Adapted from https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_serving.py

import argparse
import asyncio
import json
import logging
import os
import sys
import time
from collections import defaultdict
from datetime import datetime

import numpy as np
import pandas
from tqdm.asyncio import tqdm
from transformers import PreTrainedTokenizerBase

benchmark_path = os.environ.get("BENCHMARK_PATH")
if benchmark_path:
    sys.path.append(benchmark_path)
    print(f"Added benchmark path: {benchmark_path}")
else:
    raise EnvironmentError("BENCHMARK_PATH is not set!")

from backend_request_func import (
    ASYNC_REQUEST_FUNCS,
    OPENAI_COMPATIBLE_BACKENDS,
    RequestFuncInput,
    RequestFuncOutput,
)
from benchmark_dataset import (
    AIMODataset,
    ASRDataset,
    BenchmarkDataset,
    BurstGPTDataset,
    ConversationDataset,
    CustomDataset,
    HuggingFaceDataset,
    InstructCoderDataset,
    MTBenchDataset,
    NextEditPredictionDataset,
    RandomDataset,
    SampleRequest,
    ShareGPTDataset,
    SonnetDataset,
    VisionArenaDataset,
)
from benchmark_serving import (
    benchmark,
    calculate_metrics,
    create_argument_parser,
    get_tokenizer,
)

try:
    from vllm.utils import FlexibleArgumentParser
except ImportError:
    from argparse import ArgumentParser as FlexibleArgumentParser

logger = logging.getLogger(__name__)
SUPPORTED_ENGINES = ["vllm"]


class TraceReplayDataset(BenchmarkDataset):
    # Default values copied from benchmark_serving.py for the random dataset.
    DEFAULT_PREFIX_LEN = 0
    DEFAULT_RANGE_RATIO = 0.0
    DEFAULT_INPUT_LEN = 1024
    DEFAULT_OUTPUT_LEN = 128
    REG_GROUPS = defaultdict(list)

    def load_trace(self, trace_file):
        with open(trace_file, "r", encoding="utf-8") as f:
            # Read by line
            for line in f:
                record = json.loads(line)
                self.REG_GROUPS[int(record["timestamp"]) / 1000].append(record)
        print(f"Done load trace file, time: {time.time()}")

    def __init__(
        self,
        trace_path,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        # Load trace file
        self.load_trace(trace_path)
        trace_directory_name = os.path.dirname(trace_path)
        trace_file_name = os.path.basename(trace_path).split(".")[0]
        # Filename which prompts generated by trace file would save to
        self.prompts_file_name = (
            f"{trace_directory_name}/{trace_file_name}_dataset.jsonl"
        )

    def sample(
        self,
        tokenizer: PreTrainedTokenizerBase,
        prefix_len: int = 0,
        save_prompts: bool = False,
        **kwargs,
    ) -> dict[float, list[SampleRequest]]:
        requests = defaultdict(list)
        # If exists prompts file generated before, load prompts from file
        if os.path.exists(self.prompts_file_name):
            with open(self.prompts_file_name, "r", encoding="utf-8") as f:
                # Read by line
                for line in f:
                    record = json.loads(line)
                    timestamp = float(record["timestamp"])
                    prompt = record["prompt"]
                    input_len = record["input_length"]
                    output_length = record["output_length"]
                    requests[timestamp].append(
                        SampleRequest(
                            prompt=prompt,
                            prompt_len=input_len,
                            expected_output_len=output_length,
                        )
                    )
            print(f"Done load prompts file, time: {time.time()}")
            return requests

        assert self.REG_GROUPS is not None, "Find no trace info!!!"
        vocab_size = tokenizer.vocab_size
        num_special_tokens = tokenizer.num_special_tokens_to_add()

        all_token_sequences = []
        # Save (timestamp, output_len)
        meta_info = []
        # Generate prompts by trace file
        for timestamp, record_list in self.REG_GROUPS.items():
            for req in record_list:
                hash_ids = req["hash_ids"]
                input_length = req["input_length"]
                output_length = req["output_length"]
                prefix_token_ids = (
                    [hash_ids[i % len(hash_ids)] for i in range(prefix_len)]
                    if prefix_len > 0
                    else []
                )
                real_input_len = input_length - prefix_len - num_special_tokens
                if len(hash_ids) >= real_input_len:
                    inner_seq = hash_ids[:real_input_len]
                else:
                    offset = sum(hash_ids) % vocab_size
                    inner_seq = (
                        (offset + np.arange(real_input_len)) % vocab_size
                    ).tolist()
                token_sequence = prefix_token_ids + inner_seq

                all_token_sequences.append(token_sequence)
                meta_info.append((timestamp, output_length))

        decoded_prompts = tokenizer.batch_decode(all_token_sequences)
        print(f"Done decoded prompts, time: {time.time()}")
        re_encodeds = []
        for token_ids, prompt in zip(all_token_sequences, decoded_prompts):
            re_encodeds.append(
                tokenizer.encode(prompt, add_special_tokens=False)[: len(token_ids)]
            )
        print(f"Done reencoded prompts, time: {time.time()}")
        decoded_prompts.clear()
        decoded_prompts = tokenizer.batch_decode(re_encodeds)
        print(f"Done redecoded prompts, time: {time.time()}")
        # Write every 100 prompts to file
        batch_size = 100
        batch_data = []
        i = 0
        for (timestamp, output_length), token_ids, prompt in zip(
            meta_info, all_token_sequences, decoded_prompts
        ):
            requests[timestamp].append(
                SampleRequest(
                    prompt=prompt,
                    prompt_len=len(token_ids),
                    expected_output_len=output_length,
                )
            )
            batch_data.append(
                {
                    "timestamp": timestamp,
                    "prompt": prompt,
                    "input_length": len(token_ids),
                    "output_length": output_length,
                }
            )
            i += 1
            if save_prompts is False:
                continue
            if len(batch_data) >= batch_size or i == len(meta_info):
                with open(self.prompts_file_name, "a", encoding="utf-8") as f:
                    for data in batch_data:
                        f.write(json.dumps(data, ensure_ascii=False) + "\n")
                batch_data = []

        print(f"Done sample, time: {time.time()}")
        return requests


# Generate a request by benchmark dataset
def gene_one_req(
    req: json,
    tokenizer: PreTrainedTokenizerBase,
    args: argparse.Namespace,
    num_prompts=1,
):
    backend = args.backend

    if args.dataset_name == "custom":
        dataset = CustomDataset(dataset_path=args.dataset_path)
        input_requests = dataset.sample(
            num_requests=num_prompts,
            tokenizer=tokenizer,
            output_len=req["output_length"],
            skip_chat_template=args.custom_skip_chat_template,
        )

    elif args.dataset_name == "sonnet":
        dataset = SonnetDataset(dataset_path=args.dataset_path)
        # For the "sonnet" dataset, formatting depends on the backend.
        if args.backend == "openai-chat":
            input_requests = dataset.sample(
                num_requests=num_prompts,
                input_len=req["input_length"],
                output_len=req["output_length"],
                prefix_len=args.sonnet_prefix_len,
                tokenizer=tokenizer,
                return_prompt_formatted=False,
            )
        else:
            assert (
                tokenizer.chat_template or tokenizer.default_chat_template
            ), "Tokenizer/model must have chat template for sonnet dataset."
            input_requests = dataset.sample(
                num_requests=num_prompts,
                input_len=req["input_length"],
                output_len=req["output_length"],
                prefix_len=args.sonnet_prefix_len,
                tokenizer=tokenizer,
                return_prompt_formatted=True,
            )

    elif args.dataset_name == "hf":
        # all following datasets are implemented from the
        # HuggingFaceDataset base class
        if args.dataset_path in VisionArenaDataset.SUPPORTED_DATASET_PATHS:
            dataset_class = VisionArenaDataset
            args.hf_split = "train"
            args.hf_subset = None
        elif args.dataset_path in InstructCoderDataset.SUPPORTED_DATASET_PATHS:
            dataset_class = InstructCoderDataset
            args.hf_split = "train"
        elif args.dataset_path in MTBenchDataset.SUPPORTED_DATASET_PATHS:
            dataset_class = MTBenchDataset
            args.hf_split = "train"
        elif args.dataset_path in ConversationDataset.SUPPORTED_DATASET_PATHS:
            dataset_class = ConversationDataset
        elif args.dataset_path in AIMODataset.SUPPORTED_DATASET_PATHS:
            dataset_class = AIMODataset
            args.hf_split = "train"
        elif (
            args.dataset_path in NextEditPredictionDataset.SUPPORTED_DATASET_PATHS
        ):  # noqa: E501
            dataset_class = NextEditPredictionDataset
            args.hf_split = "train"
        elif args.dataset_path in ASRDataset.SUPPORTED_DATASET_PATHS:
            dataset_class = ASRDataset
            args.hf_split = "train"
        else:
            supported_datasets = set(
                [
                    dataset_name
                    for cls in HuggingFaceDataset.__subclasses__()
                    for dataset_name in cls.SUPPORTED_DATASET_PATHS
                ]
            )
            raise ValueError(
                f"Unsupported dataset path: {args.dataset_path}. "
                "Huggingface dataset only supports dataset_path"
                f" from one of following: {supported_datasets}. "
                "Please consider contributing if you would "
                "like to add support for additional dataset formats."
            )

        if dataset_class.IS_MULTIMODAL and backend not in [
            "openai-chat",
            "openai-audio",
        ]:
            # multi-modal benchmark is only available on OpenAI Chat backend.
            raise ValueError(
                "Multi-modal content is only supported on 'openai-chat' and "
                "'openai-audio' backend."
            )
        input_requests = dataset_class(
            dataset_path=args.dataset_path,
            dataset_subset=args.hf_subset,
            dataset_split=args.hf_split,
            random_seed=args.seed,
        ).sample(
            num_requests=num_prompts,
            tokenizer=tokenizer,
            output_len=req["output_length"],
        )

    else:
        # For datasets that follow a similar structure, use a mapping.
        dataset_mapping = {
            "sharegpt": lambda: ShareGPTDataset(
                random_seed=args.seed, dataset_path=args.dataset_path
            ).sample(
                tokenizer=tokenizer,
                num_requests=num_prompts,
                output_len=req["output_length"],
            ),
            "burstgpt": lambda: BurstGPTDataset(
                random_seed=args.seed, dataset_path=args.dataset_path
            ).sample(tokenizer=tokenizer, num_requests=num_prompts),
            "random": lambda: RandomDataset(dataset_path=args.dataset_path).sample(
                tokenizer=tokenizer,
                num_requests=num_prompts,
                prefix_len=args.random_prefix_len,
                input_len=req["input_length"],
                output_len=req["output_length"],
                range_ratio=args.random_range_ratio,
            ),
        }
        try:
            input_requests = dataset_mapping[args.dataset_name]()
        except KeyError as err:
            raise ValueError(f"Unknown dataset: {args.dataset_name}") from err
    return input_requests


# Generates prompts by dataset name
def gene_prompts_by_dataset_name(
    req_groups: dict, tokenizer: PreTrainedTokenizerBase, args: argparse.Namespace
) -> dict[float, list]:
    if args.dataset_name is None:
        raise ValueError(
            "Please specify '--dataset-name' and the corresponding "
            "'--dataset-path' if required."
        )
    # {float, list[json]}
    input_requests = defaultdict(list)
    start_time = time.perf_counter()
    for sec, reqs in sorted(req_groups.items()):
        for req in reqs:
            # Try to produce prompt by benchmark datasets
            gene_req = gene_one_req(req, tokenizer, args)
            input_requests[sec].extend(gene_req)
    gene_prompts_duration = time.perf_counter() - start_time
    print(f"Take {gene_prompts_duration} to produce prompts.")
    return input_requests


# Send requests by benchmark
async def replay_trace_by_benchmark(
    req_groups: dict, tokenizer: PreTrainedTokenizerBase, args: argparse.Namespace
):
    backend = args.backend
    model_id = args.model
    model_name = args.served_model_name
    tokenizer = args.tokenizer

    if args.base_url is not None:
        api_url = f"{args.base_url}{args.endpoint}"
        base_url = f"{args.base_url}"
    else:
        api_url = f"http://{args.host}:{args.port}{args.endpoint}"
        base_url = f"http://{args.host}:{args.port}"

    start_time = time.time()
    print(f"Start time is {start_time}")
    tasks = []
    for sec, reqs in sorted(req_groups.items()):
        delay = sec - (time.time() - start_time)
        delay = max(0, delay)

        async def send_one_request(r=reqs, d=delay):
            sampling_params = {}
            sampling_params["temperature"] = 0.9
            await asyncio.sleep(d)  # Wait until the target time
            print(f"Sending request at {time.time() - start_time:.3f}s")
            try:
                result = await benchmark(
                    backend=backend,
                    api_url=api_url,
                    base_url=base_url,
                    model_id=model_id,
                    model_name=model_name,
                    tokenizer=tokenizer,
                    input_requests=r,
                    logprobs=None,
                    request_rate=float(
                        "inf"
                    ),  # send all requests of same timestamp at once
                    burstiness=1,
                    disable_tqdm=True,
                    profile=False,
                    selected_percentile_metrics=["ttft", "tpot", "itl"],
                    selected_percentiles=[25.0, 50.0, 75.0, 99.0],
                    ignore_eos=True,
                    goodput_config_dict={},
                    max_concurrency=args.max_concurrency,
                    lora_modules=None,
                    extra_body=sampling_params,
                    ramp_up_strategy=None,
                    ramp_up_start_rps=None,
                    ramp_up_end_rps=None,
                )
            except asyncio.TimeoutError:
                print(
                    f"Request timed out: timestamp {r[0].timestamp if r else 'unknown'}"
                )
                return None
            except Exception as e:
                print(f"Request failed: {e}")
                return None
            return result

        tasks.append(asyncio.create_task(send_one_request(reqs, delay)))
    await asyncio.gather(*tasks)


def save_metrics_to_file(metrics, output_dir="./"):
    output_path = output_dir
    if not os.path.exists(output_path):
        os.makedirs(output_path, exist_ok=True)
    excel_file = os.path.join(output_path, "metrics.xlsx")

    outputs = {}
    outputs["time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    outputs["completed requests"] = metrics.completed
    outputs["request_goodput"] = metrics.request_goodput
    outputs["mean_itl(ms)"] = round(metrics.mean_itl_ms, 2)
    outputs["mean_tpot(ms)"] = round(metrics.mean_tpot_ms, 2)
    outputs["mean_ttft(ms)"] = round(metrics.mean_ttft_ms, 2)
    outputs["p99_itl(ms)"] = round(metrics.percentiles_itl_ms[3][1], 2)
    outputs["p99_tpot(ms)"] = round(metrics.percentiles_tpot_ms[3][1], 2)
    outputs["p99_ttft(ms)"] = round(metrics.percentiles_ttft_ms[3][1], 2)
    outputs["total_input_tokens"] = round(metrics.total_input, 2)
    outputs["total_output_tokens"] = round(metrics.total_output, 2)
    outputs["request_throughput(req/s)"] = round(metrics.request_throughput, 2)
    outputs["output_throughput(tok/s)"] = round(metrics.output_throughput, 2)
    outputs["total_token_throughput(tok/s)"] = round(metrics.total_token_throughput, 2)

    df = pandas.DataFrame([outputs])
    if os.path.isfile(excel_file):
        try:
            existing_df = pandas.read_excel(excel_file)
            updated_df = pandas.concat([existing_df, df], ignore_index=True)
        except Exception as e:
            print(
                f"Warning: Failed to read {excel_file}, it will be overwritten. Error: {e}"
            )
            updated_df = df
    else:
        updated_df = df
    # Save back to Excel (automatically create or overwrite)
    with pandas.ExcelWriter(excel_file, engine="openpyxl", mode="w") as writer:
        updated_df.to_excel(writer, index=False, sheet_name="Performance Metrics")
    print(f"Successfully saved performance metrics to {excel_file}")


# Send requests by timestamp
async def replay_trace_by_time(
    req_groups: dict, tokenizer: PreTrainedTokenizerBase, args: argparse.Namespace
):
    backend = args.backend
    model_id = args.model
    model_name = args.served_model_name
    disable_tqdm = args.disable_tqdm

    if args.base_url is not None:
        api_url = f"{args.base_url}{args.endpoint}"
        base_url = f"{args.base_url}"
    else:
        api_url = f"http://{args.host}:{args.port}{args.endpoint}"
        base_url = f"http://{args.host}:{args.port}"

    if backend not in ASYNC_REQUEST_FUNCS:
        raise ValueError(f"Unknown backend: {backend}")
    request_func = ASYNC_REQUEST_FUNCS[backend]

    print("Starting initial single prompt test run...")
    test_request = None
    for _, reqs in sorted(req_groups.items()):
        if reqs:
            test_request = reqs[0]
            break
    if test_request is None:
        raise ValueError("No request found for initial test run.")
    test_input = RequestFuncInput(
        model=model_id,
        model_name=model_name,
        prompt=test_request.prompt,
        api_url=api_url,
        prompt_len=test_request.prompt_len,
        output_len=test_request.expected_output_len,
        logprobs=None,
        multi_modal_content=getattr(test_request, "multi_modal_data", None),
        ignore_eos=True,
        extra_body={"temperature": 0.9},
    )

    test_output = await request_func(request_func_input=test_input)
    print("/////////////////////////////////////////////////////////////////////")
    print(test_output)
    if not getattr(test_output, "success", False):
        raise ValueError(
            "Initial test run failed - Please make sure arguments "
            f"are correctly specified. Error: {getattr(test_output, 'error', '')}"
        )
    else:
        print("Initial test run completed. Starting main run...")

    total = sum(len(req_list) for req_list in req_groups.values())
    pbar = None if disable_tqdm else tqdm(total=total)
    semaphore = (
        asyncio.Semaphore(args.max_concurrency)
        if getattr(args, "max_concurrency", None)
        else None
    )
    start_time = time.perf_counter()
    print(f"Start time is {start_time}")
    tasks = []
    flat_requests = []

    async def _run_one_request(sample_req):
        sampling_params = {"temperature": 0.9}
        req_input = RequestFuncInput(
            model=model_id,
            model_name=model_name,
            prompt=sample_req.prompt,
            api_url=api_url,
            prompt_len=sample_req.prompt_len,
            output_len=sample_req.expected_output_len,
            logprobs=None,
            extra_body=sampling_params,
            ignore_eos=True,
        )
        if semaphore is not None:
            async with semaphore:
                return await request_func(request_func_input=req_input, pbar=pbar)
        else:
            return await request_func(request_func_input=req_input, pbar=pbar)

    for sec, reqs in sorted(req_groups.items()):
        delay = sec - (time.perf_counter() - start_time)
        delay = max(0, delay)
        print(f"timestamp is {sec}, length of reqs is {len(reqs)}")

        async def send_group(r=reqs, d=delay):
            await asyncio.sleep(d)
            print(
                f"Sending request at {time.perf_counter() - start_time:.3f}s with {len(r)} reqs"
            )
            group_tasks = [asyncio.create_task(_run_one_request(req)) for req in r]
            try:
                return await asyncio.gather(*group_tasks)
            except asyncio.TimeoutError:
                print(f"Request timed out: group at delay {d:.3f}s")
                return []
            except Exception as e:
                print(f"Request failed: {e}")
                return []

        tasks.append(asyncio.create_task(send_group(reqs, delay)))
        flat_requests.extend(reqs)

    group_results = await asyncio.gather(*tasks)
    outputs = []
    for res in group_results:
        if isinstance(res, list):
            outputs.extend(res)

    if pbar is not None:
        pbar.close()

    benchmark_duration = time.perf_counter() - start_time
    metrics, actual_output_lens = calculate_metrics(
        input_requests=flat_requests,
        outputs=outputs,
        dur_s=benchmark_duration,
        tokenizer=tokenizer,
        selected_percentile_metrics=["ttft", "tpot", "itl", "e2el"],
        selected_percentiles=[25.0, 50.0, 75.0, 99.0],
        goodput_config_dict={"ttft": 2000, "tpot": 50},
    )

    if args.save_result:
        save_metrics_to_file(metrics=metrics)

    print("{s:{c}^{n}}".format(s=" Serving Benchmark Result ", n=50, c="="))
    print("{:<40} {:<10}".format("Successful requests:", metrics.completed))
    print("{:<40} {:<10.2f}".format("Benchmark duration (s):", benchmark_duration))
    print("{:<40} {:<10}".format("Total input tokens:", metrics.total_input))
    print("{:<40} {:<10}".format("Total generated tokens:", metrics.total_output))
    print(
        "{:<40} {:<10.2f}".format(
            "Request throughput (req/s):", metrics.request_throughput
        )
    )
    print(
        "{:<40} {:<10.2f}".format(
            "Output token throughput (tok/s):", metrics.output_throughput
        )
    )
    print(
        "{:<40} {:<10.2f}".format(
            "Total Token throughput (tok/s):", metrics.total_token_throughput
        )
    )

    # Define the process_one_metric function, which can access the outer scope's selected_percentile_metrics
    def process_one_metric(
        metric_attribute_name: str,
        metric_name: str,
        metric_header: str,
    ):
        selected_percentile_metrics = ["ttft", "tpot", "itl", "e2el"]
        if metric_attribute_name not in selected_percentile_metrics:
            return
        print("{s:{c}^{n}}".format(s=metric_header, n=50, c="-"))
        print(
            "{:<40} {:<10.2f}".format(
                f"Mean {metric_name} (ms):",
                getattr(metrics, f"mean_{metric_attribute_name}_ms"),
            )
        )
        print(
            "{:<40} {:<10.2f}".format(
                f"Median {metric_name} (ms):",
                getattr(metrics, f"median_{metric_attribute_name}_ms"),
            )
        )
        # standard deviation
        print(
            "{:<40} {:<10.2f}".format(
                f"Std {metric_name} (ms):",
                getattr(metrics, f"std_{metric_attribute_name}_ms"),
            )
        )
        for p, value in getattr(metrics, f"percentiles_{metric_attribute_name}_ms"):
            p_word = str(int(p)) if int(p) == p else str(p)
            print("{:<40} {:<10.2f}".format(f"P{p_word} {metric_name} (ms):", value))

    process_one_metric("ttft", "TTFT", "Time to First Token")
    process_one_metric("tpot", "TPOT", "Time per Output Token (excl. 1st token)")
    process_one_metric("itl", "ITL", "Inter-token Latency")
    process_one_metric("e2el", "E2EL", "End-to-end Latency")
    print("=" * 50)

    if args.save_result:
        for i, output in enumerate(outputs):
            save_single_metrics_to_file(output=output, output_dir="./")

    return


def save_single_metrics_to_file(output, output_dir="./"):
    ttft = None
    tpot = None
    success = False
    output_len = None
    success = output.success
    ttft = output.ttft * 1000 if output.ttft is not None else None
    output_len = output.output_tokens if output.output_tokens is not None else 0
    input_len = output.prompt_len if output.prompt_len is not None else 0
    latency = output.latency * 1000 if output.latency is not None else None
    if output_len > 1:
        tpot = (output.latency - output.ttft) / (output_len - 1) * 1000

    output_path = output_dir
    if not os.path.exists(output_path):
        os.makedirs(output_path, exist_ok=True)
    excel_file = os.path.join(output_path, "single_metrics.xlsx")

    outputs = {}
    outputs["time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    outputs["tpot(ms)"] = tpot
    outputs["ttft(ms)"] = ttft
    outputs["e2el(ms)"] = latency
    outputs["output_tokens"] = output_len
    outputs["input_tokens"] = input_len
    outputs["success"] = success

    df = pandas.DataFrame([outputs])
    file_exists = os.path.isfile(excel_file)
    if file_exists:
        try:
            existing_df = pandas.read_excel(excel_file)
            updated_df = pandas.concat([existing_df, df], ignore_index=True)
        except Exception as e:
            print(
                f"Warning: Failed to read {excel_file}, it will be overwritten. Error: {e}"
            )
            updated_df = df
    else:
        updated_df = df
    # Save back to Excel (automatically create or overwrite)
    with pandas.ExcelWriter(
        excel_file,
        engine="openpyxl",
        mode="a" if file_exists else "w",
        if_sheet_exists="replace",
    ) as writer:
        updated_df.to_excel(writer, index=False, sheet_name="Performance Metrics")


def create_argument_trace():
    parser = create_argument_parser()
    trace_group = parser.add_argument_group("tracing parameters")
    trace_group.add_argument(
        "--trace-path",
        type=str,
        default=None,
        help="Path to trace file path.",
    )
    trace_group.add_argument(
        "--trace-mode",
        type=str,
        default="trace",
        choices=["trace", "benchmark"],
        help="Specify the trace mode: 'trace' to replay requests from cached trace files, or 'benchmark' to generate requests dynamically using the benchmark module.",
    )
    trace_group.add_argument(
        "--save-prompts",
        action="store_true",
        help="Save generated prompts with timestamp for reuse.",
    )
    return parser


def main(args: argparse.Namespace):
    print(args)
    dataset = TraceReplayDataset(args.trace_path)

    tokenizer_id = args.tokenizer if args.tokenizer is not None else args.model
    tokenizer_mode = args.tokenizer_mode
    tokenizer = get_tokenizer(
        tokenizer_id,
        tokenizer_mode=tokenizer_mode,
        trust_remote_code=args.trust_remote_code,
    )

    # Generate prompts
    if args.trace_mode == "trace":
        input_requests = dataset.sample(
            tokenizer=tokenizer, save_prompts=args.save_prompts
        )
    else:
        input_requests = gene_prompts_by_dataset_name(
            req_groups=dataset.REG_GROUPS, tokenizer=tokenizer, args=args
        )

    # Send prompts by timestamp
    asyncio.run(replay_trace_by_time(input_requests, tokenizer, args))


if __name__ == "__main__":
    parser = create_argument_trace()
    args = parser.parse_args()
    main(args)
