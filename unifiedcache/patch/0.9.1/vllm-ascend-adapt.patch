From e45ed500c23f3b8905c68ada894657fd0794906b Mon Sep 17 00:00:00 2001
From: y00945504 <yuhui87@huawei.com>
Date: Fri, 22 Aug 2025 11:46:48 +0800
Subject: [PATCH] manually apply patch

---
 vllm_ascend/attention/attention_v1.py | 33 +++++++++++++++++++++++++++
 vllm_ascend/worker/model_runner_v1.py | 14 +++++++-----
 2 files changed, 41 insertions(+), 6 deletions(-)

diff --git a/vllm_ascend/attention/attention_v1.py b/vllm_ascend/attention/attention_v1.py
index 694adab..487b12b 100644
--- a/vllm_ascend/attention/attention_v1.py
+++ b/vllm_ascend/attention/attention_v1.py
@@ -24,6 +24,9 @@ import torch_npu
 from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
                                               AttentionLayer, AttentionType)
 from vllm.attention.backends.utils import CommonAttentionState
+from vllm.distributed.kv_transfer import (get_kv_transfer_group,
+                                          has_kv_transfer_group,
+                                          is_v1_kv_transfer_group)
 from vllm.config import get_current_vllm_config
 from vllm.forward_context import ForwardContext, get_forward_context
 from vllm.utils import direct_register_custom_op
@@ -458,6 +461,8 @@ def unified_ascend_attention_with_output(
     output: torch.Tensor,
     layer_name: str,
 ) -> None:
+    wait_for_kv_layer_from_connector(layer_name)
+
     forward_context: ForwardContext = get_forward_context()
     attn_metadata = forward_context.attn_metadata
     self = forward_context.no_compile_layers[layer_name]
@@ -470,8 +475,36 @@ def unified_ascend_attention_with_output(
                       attn_metadata,
                       output,
                       trace_flag=False)
+    maybe_save_kv_layer_to_connector(layer_name, kv_cache)
     return
 
+def wait_for_kv_layer_from_connector(layer_name: str):
+    if not has_kv_transfer_group() or not is_v1_kv_transfer_group():
+        return
+
+    connector = get_kv_transfer_group()
+
+    forward_context: ForwardContext = get_forward_context()
+    attn_metadata = forward_context.attn_metadata
+    if attn_metadata is None:
+        return
+    connector.wait_for_layer_load(layer_name)
+
+def maybe_save_kv_layer_to_connector(
+    layer_name: str,
+    kv_cache_layer: List[torch.Tensor],
+):
+    if not has_kv_transfer_group() or not is_v1_kv_transfer_group():
+        return
+
+    connector = get_kv_transfer_group()
+
+    forward_context: ForwardContext = get_forward_context()
+    attn_metadata = forward_context.attn_metadata
+    if attn_metadata is None:
+        return
+    connector.save_kv_layer(layer_name, kv_cache_layer,
+                            attn_metadata)
 
 def unified_attention_with_output_fake(
     query: torch.Tensor,
diff --git a/vllm_ascend/worker/model_runner_v1.py b/vllm_ascend/worker/model_runner_v1.py
index dc28bfa..ddc996b 100644
--- a/vllm_ascend/worker/model_runner_v1.py
+++ b/vllm_ascend/worker/model_runner_v1.py
@@ -889,7 +889,7 @@ class NPUModelRunner(LoRAModelRunnerMixin):
         intermediate_tensors: Optional[IntermediateTensors] = None,
     ) -> tuple[SpecDecodeMetadata, torch.Tensor, SpecDecodeMetadata,
                torch.Tensor, int, torch.Tensor, Optional[set[str]],
-               Optional[set[str]]]:
+               Optional[set[str]], Optional[dict[str, list[str]]]]:
         # Check input valid
         total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens
         assert total_num_scheduled_tokens > 0
@@ -1140,6 +1140,7 @@ class NPUModelRunner(LoRAModelRunnerMixin):
             positions = self.positions[:padded_num_tokens_across_dp]
 
         # Run forward pass
+        finished_dumping = None
         # TODO(zzzzwwjj): check param `num_tokens_across_dp` later.
         with set_ascend_forward_context(
                 attn_metadata,
@@ -1174,7 +1175,7 @@ class NPUModelRunner(LoRAModelRunnerMixin):
                         inputs_embeds=inputs_embeds,
                         **model_kwargs)
 
-        self.maybe_wait_for_kv_save()
+        finished_dumping = self.maybe_wait_for_kv_save()
         finished_sending, finished_recving = self.get_finished_kv_transfer(
             scheduler_output)
         use_spec_decode = len(
@@ -1202,7 +1203,7 @@ class NPUModelRunner(LoRAModelRunnerMixin):
 
         return (attn_metadata, hidden_states, spec_decode_metadata, positions,
                 total_num_scheduled_tokens, sample_indices, finished_sending,
-                finished_recving)
+                finished_recving, finished_dumping)
 
     def _calc_spec_decode_metadata(
         self,
@@ -1386,7 +1387,7 @@ class NPUModelRunner(LoRAModelRunnerMixin):
 
             (attn_metadata, hidden_states, spec_decode_metadata, positions,
              num_scheduled_tokens, sample_indices, finished_sending,
-             finished_recving) = (self._process_reqs(scheduler_output,
+             finished_recving, finished_dumping) = (self._process_reqs(scheduler_output,
                                                      intermediate_tensors))
 
             if self.dynamic_eplb:
@@ -1493,6 +1494,7 @@ class NPUModelRunner(LoRAModelRunnerMixin):
                 prompt_logprobs_dict={},
                 finished_sending=finished_sending,
                 finished_recving=finished_recving,
+                finished_dumping=finished_dumping
             )
 
         durations = ProfileExecuteDuration().pop_captured_sync()
@@ -1543,8 +1545,8 @@ class NPUModelRunner(LoRAModelRunnerMixin):
     @staticmethod
     def maybe_wait_for_kv_save() -> None:
         if has_kv_transfer_group():
-            get_kv_transfer_group().wait_for_save()
-
+            return get_kv_transfer_group().wait_for_save()
+            
     @staticmethod
     def get_finished_kv_transfer(
         scheduler_output: "SchedulerOutput",
-- 
2.50.1.windows.1

